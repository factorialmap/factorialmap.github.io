[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Info",
    "section": "",
    "text": "Goals\nO objetivo do blog é complementar o conteúdo postado no YouTube e tentar facilitar a vida do pesquisador, melhorista de processo e demais profissionais da industria e setor de serviços.\n\n\n Education\nNova Acrópole | Florianópolis, SC, Brasil\nFilosofia na tradição Clássica | 2014-2015\nUniversidade Uniderp | Campo Grande, MS, Brasil\nMestrado Produção e Gestão Agroindustrial | 2008-2010"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Carvalho Ribeiro Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nClassificação de AI\n\n\nClasses de AI para clareza conceitual\n\n\n\nDemanda ou problema\n\n\n\n\n\n\n\n\n\nAug 21, 2025\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nProblema de 2 sigma de Bloom\n\n\nCrianças aprendem mais e outras menos\n\n\n\nDemanda ou problema\n\n\n\n\n\n\n\n\n\nAug 14, 2025\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nVisualização de dados usando Python\n\n\n\n\n\n\nExploração de dados\n\n\n\n\n\n\n\n\n\nOct 22, 2024\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretando Análise de componentes principais PCA\n\n\n\n\n\n\nExploração de dados\n\n\n\n\n\n\n\n\n\nAug 15, 2024\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nImpressão de tabelas em formatos pdf e html em Quarto documents\n\n\n\n\n\n\nComunicação\n\n\n\n\n\n\n\n\n\nAug 11, 2024\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nFormatando tabelas e dados usando themes e gtsummary package no R\n\n\n\n\n\n\nComunicação\n\n\n\n\n\n\n\n\n\nJun 16, 2024\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nFormatar tabelas para publicar artigos no R com GT package\n\n\n\n\n\n\nComunicação\n\n\n\n\n\n\n\n\n\nMay 6, 2024\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nDiagrama de Causa e Efeito\n\n\n\n\n\n\nDemanda ou problema\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nModelo preditivo | Amostra e reamostragens\n\n\nFunções úteis split de dados e resample que serão usados na validação e ajuste de modelos\n\n\n\nModelos\n\n\n\n\n\n\n\n\n\nSep 29, 2023\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nComunicar trabalhos técnicos em apresentações no R com Quarto\n\n\nComunicar resultados no formato apresentação de slides em reveljs\n\n\n\nComunicação\n\n\n\n\n\n\n\n\n\nJul 16, 2023\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nModelo preditivo | tidymodels decision tree rpart diabetes\n\n\nModelo machine learning de árvore de decisão usando rpart, tidymodels e dados de diabetes\n\n\n\nModelos\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nComo comunicar dados em artigos técnicos no R - Quarto\n\n\n\n\n\n\nComunicação\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nComo construir gráficos para artigo técnico\n\n\n\n\n\n\nExploração de dados\n\n\nComunicação\n\n\n\n\n\n\n\n\n\nSep 28, 2022\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nComo construir tabela para artigo técnico - gtsummary\n\n\n\n\n\n\nComunicação\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nComo Identificar padrões em dados - gráfico de dispersão\n\n\n\n\n\n\nExploração de dados\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nComo analisar distribuição em dados - histograma curva normal\n\n\n\n\n\n\nExploração de dados\n\n\n\n\n\n\n\n\n\nAug 21, 2022\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nComo importar e visualizar dados - Titanic dataset parte1\n\n\n\n\n\n\nExploração de dados\n\n\n\n\n\n\n\n\n\nAug 7, 2022\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nModelo preditivo usando com rede neural\n\n\n\n\n\n\nModelos\n\n\n\n\n\n\n\n\n\nAug 6, 2022\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nAvaliar resultados de modelo preditivo com matriz de confusão\n\n\n\n\n\n\nModelos\n\n\n\n\n\n\n\n\n\nAug 3, 2022\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\n\n\n\n\n\n\nMotivação para o blog\n\n\n\n\n\n\nDemanda ou problema\n\n\n\n\n\n\n\n\n\nAug 3, 2022\n\n\nMarcelo Carvalho dos Anjos\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/chart_stats/plot_stats.html",
    "href": "posts/chart_stats/plot_stats.html",
    "title": "Como construir gráficos para artigo técnico",
    "section": "",
    "text": "Vídeo tema para este post em Como construir gráficos para artigo técnico\n\n\n\n\n O que é um gráfico\n\nUm meio eficiente de representação e comunicação de dados e pode conter uma quantidade de significativa de informação.\nFormato visual de representação de dados que é interpretado em grande velocidade pelo cerebro e que permite ações ou decisões mais rápidas.\n\n\n\n\nAlguns tipos de gráficos\n\n\n\n\n Qual o objetivo\n\nFacilitar a identificação de padrões, tendências e discrepancias nos dados transformando em informações.\nPermitir ações imediatas ou gerar informação complementar para que ações possam ser tomadas posteriormente.\n\n\n\n De onde vem a demanda\n\nNecessidade de representação de dados que potencialize o desenvolvimento de estratégias e ações.\nNecessidade de algo que fortaleça o argumento em um estudo ou pesquisa.\n\n\n\n Como fazer\nPara reproduzir os códigos abaixo serão necessários os pacotes tidyverse , ggpmisc, janitor, qcc, ggpubr, ggQC, quantmod e ggalluvial\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(ggpmisc)\nlibrary(qcc)\nlibrary(ggQC)\nlibrary(quantmod)\nlibrary(easyalluvial)\n\nGráfico de comparação\n\ngroup_compar1 &lt;- \n  list(c(\"4\",\"6\"),\n       c(\"6\",\"8\"),\n       c(\"4\",\"8\"))\n\nmtcars %&gt;% \n  ggboxplot(x = \"cyl\",\n            y= \"mpg\",\n            fill = \"cyl\",\n            palette = \"uchicago\")+\n  stat_compare_means(comparisons = group_compar1)\n\n\n\n\n\n\n\n\nPlotando estatístics no corpo do gráfico.\n\nmtcars %&gt;% \n  ggplot(aes(x = wt, y = mpg))+\n  geom_point()+\n  stat_fit_tb(method = \"lm\",\n              label.x = \"right\",\n              method.args = list(formula = y~x),\n              table.theme = ttheme_gtdark())\n\n\n\n\n\n\n\n\nPlotando fórmulas no corpo do gráfico.\n\nmtcars %&gt;% \n  ggplot(aes(x = wt, y = mpg))+\n  geom_point()+\n  stat_correlation(use_label(c(\"r\", \"t\", \"p\")),\n                   label.x = \"right\")\n\n\n\n\n\n\n\n\nPlotando dados no corpo do gráfico.\n\ntbl_median_mtcars &lt;- \n  mtcars %&gt;% \n  select(mpg, hp, cyl) %&gt;% \n  group_by(cyl) %&gt;% \n  summarise(across(where(is.numeric), median)) %&gt;% \n  ungroup()\n\ndata_trans_mtcars &lt;- tibble(x = 5.40, y = 34, tb = list(tbl_median_mtcars))\n\nmtcars %&gt;% \n  ggplot(aes(x = wt, y = mpg, color = factor(cyl)))+\n  geom_point()+\n  geom_table(data= data_trans_mtcars, aes(x=x, y=y, label = tb))\n\n\n\n\n\n\n\n\nGráfico de cause efeito.\n\nqcc::cause.and.effect(\n  cause = list(\n    medida = c(\"metrica errada\"),\n    maquin = c(\"defeito\"),\n    metodo = c(\"treinamento\")\n      ),\n  effect = \"efeito\"\n)\n\n\n\n\n\n\n\n\nModelo para auxiliar na escolha de gráfico de controle.\n\n\n\n\n\nflowchart LR\n    A[dados] --&gt;B{qual tipo \\n de dados?}\n    B --&gt;C[Contínuo]\n    B --&gt;D[Discreta/\\nAtributos]\n    C --&gt;E{Qual o volume\\n de dados ?}\n    E --&gt;|n = 1|F[x.one + R]\n    E --&gt;|n 2-10|G[x + R]\n    E --&gt;|n &gt; 10|H[x + S]\n    D --&gt;I{Lote de dados\\n constante ?}\n    I--&gt;J[Sim]\n    I--&gt;K[Não]\n    J--&gt;L[C Proporção de \\ndefeitos por UN]\n    J--&gt;M[NP Contagem de \\ndefeitos por UN]\n    K--&gt;N[U Contagem de \\ndefeitos por UN]\n    K--&gt;O[P Proporção de \\ndefeitos por UN]\n\n\n\n\n\n\nGráfico X bar de controle de processo.\n\ndata(\"pistonrings\")\n\ndata_pistonring &lt;- as_tibble(pistonrings)\n\nqcc(data_pistonring$diameter,\n                type = \"xbar.one\",\n                rules = shewhart.rules)\n\n\n\n\n\n\n\n\nList of 11\n $ call      : language qcc(data = data_pistonring$diameter, type = \"xbar.one\", rules = shewhart.rules)\n $ type      : chr \"xbar.one\"\n $ data.name : chr \"data_pistonring$diameter\"\n $ data      : num [1:200, 1] 74 74 74 74 74 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ statistics: Named num [1:200] 74 74 74 74 74 ...\n  ..- attr(*, \"names\")= chr [1:200] \"1\" \"2\" \"3\" \"4\" ...\n $ sizes     : int [1:200] 1 1 1 1 1 1 1 1 1 1 ...\n $ center    : num 74\n $ std.dev   : num 0.01\n $ nsigmas   : num 3\n $ limits    : num [1, 1:2] 74 74\n  ..- attr(*, \"dimnames\")=List of 2\n $ violations:List of 2\n - attr(*, \"class\")= chr \"qcc\"\n\n\nGráfico de pareto.\n\ndata_defeito &lt;- \n  tribble(~\"tipo\", ~\"qtd\", ~\"custo\",\n          \"riscos\", 201, 4287.60,\n          \"Manchas\",78,2423.46,\n          \"Dobras\",47,1118.60,\n          \"Furos\",31,8946.60,\n          \"Rasgos\",15,1864.05)\n\npareto_defeito &lt;- \n  data_defeito %&gt;% \n  ggplot(aes(x = tipo, y= qtd))+\n  stat_pareto(bars.fill = \"lightblue\")\n\npareto_custo &lt;- \n  data_defeito %&gt;% \n  ggplot(aes(x = tipo, y= custo))+\n  stat_pareto(bars.fill = \"navy\")\n\ndata_defeito %&gt;% \n  ggplot(aes(x = tipo, y = custo))+\n  stat_pareto(bars.fill = \"navy\")+\n  annotate(\"plot_npc\", \n            npcx = \"right\", \n            npcy = \"middle\",\n            label =pareto_defeito )\n\n\n\n\n\n\n\n\nGráfico de sentimento candlestick chart.\n\ngetSymbols(\"MORTGAGE30US\", src= \"FRED\")\n\n[1] \"MORTGAGE30US\"\n\nchart_Series(MORTGAGE30US)\n\n\n\n\n\n\n\ngetSymbols(\"VIXCLS\", src= \"FRED\")\n\n[1] \"VIXCLS\"\n\nchart_Series(VIXCLS)\n\n\n\n\n\n\n\ngetSymbols(\"DGS10\", src= \"FRED\")\n\n[1] \"DGS10\"\n\nchart_Series(DGS10)\n\n\n\n\n\n\n\ngetSymbols(\"VALE\")\n\n[1] \"VALE\"\n\nchartSeries(VALE)\n\n\n\n\n\n\n\n\nGráfico de análise multidimensional Alluvial.\n\nplot_mtcars &lt;- \n  mtcars %&gt;% \n  alluvial_wide(max_variables = 5) \n  \nadd_marginal_histograms(plot_mtcars, mtcars)\n\n\n\n\n\n\n\n\nGráfico PCA\n\niris %&gt;% \n  select(-Species) %&gt;% \n  prcomp(center = TRUE, scale. = TRUE) %&gt;% \n  factoextra::fviz_pca_biplot(geom.ind = \"point\",\n                              habillage = iris$Species,\n                              addEllipses = TRUE)\n\n\n\n\n\n\n\n\n\n\n Pra onde vai quem é o cliente\n\nEtapas de analise de resultados, validação, modelagem ou comunicação.\nInclusão em documento técnico reproduzível.\nComunicação de descobertas ou resultados de trabalhos técnicos\nDocumentação ou treinamento de pessoal.\n\n\n\n Qual o resultado\n\nAperfeiçoar as técnicas de representação, exploração e comunicação de estudos técnicos.\nFacilitar o entendimento das informações contidas nos documentos técnicos poupando tempo e dinheiro, permitindo a replicação e experimentos e consequentemente busca por melhoria.\nFacilitar o reconhecimento de registros, estudos, descobertas em pesquisas futuras."
  },
  {
    "objectID": "posts/confusion-matrix/index.html",
    "href": "posts/confusion-matrix/index.html",
    "title": "Avaliar resultados de modelo preditivo com matriz de confusão",
    "section": "",
    "text": "Vídeo tema para este post em Avaliar resultados de modelo preditivo com matriz de confusão\n\n\n\n\n O que é uma matriz de confusão\n\nTabela que facilita a visualização das previsões corretas e as erradas em modelos de classificação.\nPossibilita a interpretação dos resultados sobre diversos pontos de vista desde que tenhamos os valores reais e os valores previstos pelo modelo.\n\n\n\nMatriz de confusão\n\n\n\nTP - verdadeiros positivos: Casos em que previmos que o paciênte tem a doença e ele realmente tem.\nTN - verdadeiro negativos: Casos em que previmos que o paciênte não tem a doença e ele realmente não tem.\nFP - falso positivo: Previmos que sim, mas na verdade ele não tem a doença. também conhecido como “erro do tipo 1”.\nFN - falso negativo: Previmos que não, mas na verdade ele tem a doença. também conhecido como “erro do tipo 2”.\n\n\n\n\n Qual o objetivo\n\nPor ser visual ajuda a responder rapidamente será que meu modelo teve um bom desempenho, onde deu errado, como posso corrigir ?\nAs diversas saídas de previõe permite a criação de indicadores que serão uteis nos ajustes mais adequados conforme a necessidade do trabalho que está sendo desenvolvido.\n\n\n\n De onde vem a demanda\n\nEm 1904 Karl Pearson criou a tabela de contingência. Por que precisamos e uma matriz de confusao se tevem a acuracidade ?\nImagine prever quantas pessoas estao infectadas com um virus contagioso antes de apresentar sintomas e isola-las da populacao saudavel.\n\n\n\n Como fazer\nPara reproduzir os códigos abaixo serão necessários os pacotes tidyverse , tidymodels e janitor\nO principal pacote que será usando para analisar uma matriz de confusão é o pacote yardstick que já é carregado quando chamamos o pacote tidymodels.\n\n#packages\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n#check\ntwo_class_example |&gt;  filter(Class1 &gt;0.5)  |&gt;  janitor::tabyl(truth)\n\n  truth   n   percent\n Class1 227 0.8194946\n Class2  50 0.1805054\n\n#exemplo de tabela cruzada usando janitor\ntwo_class_example |&gt;  \n  janitor::tabyl(truth, predicted) |&gt;  \n  janitor::adorn_totals(where = c(\"col\", \"row\"))  |&gt;  \n  janitor::adorn_title()\n\n        predicted             \n  truth    Class1 Class2 Total\n Class1       227     31   258\n Class2        50    192   242\n  Total       277    223   500\n\n#trocando nomes\ntwo_class_example |&gt;\n  conf_mat(truth =truth, estimate =predicted, dnn =c(\"vlr_previsto\",\"vlr_real\"))\n\n            vlr_real\nvlr_previsto Class1 Class2\n      Class1    227     50\n      Class2     31    192\n\n#plot heatmap\ntwo_class_example |&gt;\n  conf_mat(truth = truth, estimate = predicted) |&gt;\n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\n#acuracia- prop de acertos do modelo total de acerto / total previu\ntwo_class_example |&gt;\n  accuracy(truth= truth, estimate = predicted)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.838\n\n#sensibilidade ou recall é a proporção de casos positivos classif corretamente\n#Raio X de aeroporto prioriza sensitivity\n#diagnóstico de cancer também pois o não diagnóstico resulta em atraso no tratamento\ntwo_class_example |&gt;\n  yardstick::sens(truth= truth, estimate = predicted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.880\n\n#Recall métrica útil nos casos em que o Falso Negativo supera o falso positivol\n#importante em casos médicos em que não importa se disparamos um alarme \n#falso, mas os casos positivos reais não devem passar despercebidos!\n#Em nosso exemplo, Recall seria uma métrica melhor porque não queremos dar alta \n#acidentalmente a uma pessoa infectada e deixá-la se misturar com a população \n#saudável,espalhando o vírus contagioso. \n#Agora você pode entender por que a acuracidade foi uma métrica ruim para modelo.\ntwo_class_example |&gt;\n  recall(truth= truth, estimate = predicted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 recall  binary         0.880\n\n#kappa\ntwo_class_example |&gt;\n  kap(truth= truth, estimate = predicted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 kap     binary         0.675\n\n#npv\ntwo_class_example |&gt;\n  npv(truth= truth, estimate = predicted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 npv     binary         0.861\n\n#ppv\ntwo_class_example |&gt;\n  ppv(truth= truth, estimate = predicted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 ppv     binary         0.819\n\n#A precisão é util nos casos em que os falsos positivos são uma preocupação \n#maior do que os falsos negativos.\n#A precisão é importante em sistemas de recomendação de música ou vídeo, \n#sites de comércio eletrônico, etc. Resultados errados podem levar à perda de \n#clientes e prejudicar o negócio.\ntwo_class_example |&gt;\n  precision(truth= truth, estimate = predicted)\n\n# A tibble: 1 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 precision binary         0.819\n\n#spec\ntwo_class_example |&gt;\n  spec(truth= truth, estimate = predicted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 spec    binary         0.793\n\n#roc_curve\ntwo_class_example |&gt;\n  roc_curve(truth, Class1 ) |&gt; autoplot()\n\n\n\n\n\n\n\n#roc_auc\ntwo_class_example |&gt;\n  roc_auc(truth, Class1 )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.939\n\n\n\n\n\n\n\n\nDiferença entre incidência e prevalencia\n\n\n\n\n\nIncidencia é o número de casos recém-diagnosticados de uma doença. Número de novos casos de uma doença dividido pelo número de pessoas em risco para a doença. Se, ao longo de um ano, cinco mulheres são diagnosticadas com câncer de mama, de uma população total de 200 mulheres do estudo (que não têm câncer de mama no início do período de estudo), então diríamos que a incidência de câncer de mama câncer nesta população foi de 0,025. (ou 2.500 por 100.000 mulheres-anos de estudo)\nPrevalência é o número total de casos da doença existentes em uma população. A taxa de prevalência é o número total de casos de uma doença existente em uma população dividido pela população total. Assim, se uma medida de câncer é feita em uma população de 40.000 pessoas e 1.200 foram recentemente diagnosticadas com câncer e 3.500 estão vivendo com câncer, então a prevalência de câncer é de 0,118. (ou 11.750 por 100.000 pessoas)\nFonte: Departament of health - NY State at https://www.health.ny.gov/diseases/chronic/basicstat.htm\n\n\n\n\n\n Pra onde vai quem é o cliente\n\nA próxima etapa é o ajuste, finalização e comunicação do modelo.\n\n\n\n Qual o resultado\n\nAperfeiçoar as técnicas de avaliação de performance de modelos poupando tempo e dinheiro.\nFacilitar o processo de busca por melhorias nos modelos de previsão.\nMelhorar a comunicação dos resultados."
  },
  {
    "objectID": "posts/histogram/histogram.html",
    "href": "posts/histogram/histogram.html",
    "title": "Como analisar distribuição em dados - histograma curva normal",
    "section": "",
    "text": "Vídeo tema para este post em Como analisar distribuição em dados - histograma curva normal\n\n\n\n\n O que é histograma\n\nGráfico que mostra como os dados coletados (amostra) estão distribuidos. Também mostra a média e como esses dados estão distante da média (desvio padrão e variância).\n\n\n\n\nHistograma\n\n\n\n\n Qual o objetivo\n\nObter uma resposta racional que auxilie na decisão (pre processamento, estudo do outlier, escolha do modelo mais adequado, identificação distorções na realidade).\n\n\n\n De onde vem a demanda\nDe questões como:\n\nComo está a distribuição de renda da população ?\nComo está a distribuição de peso na minha plantação de tomates?\nComo está a distribuição de altura das pessoas em determinada região?\nSerá que se eu separar em grupos eu consigo atender melhor as necessidades ?\n\n\n\n\n\n\n\nClique aqui pra abrir os detalhes\n\n\n\n\n\n . Imagine que eu queira prever o peso dos cachorros com base nas características deles.\n . Para explicar a distribuição dos pesos, tento dividir a população em grupos com base nessas características.\n . Em um agrupamento bem sucedido, os grupos terão baixa variação dentro do grupo e boa variação entre os grupos.\n . Na primeira tentativa, escolho jovem pelo curto, jovem pelo longo, velho pelo curto, jovem pelo longo.\n . Na segunda tentantiva escolho cães de trabalho atletico, trabalho faro, domestico atletico, domestico faro.\n . Na terceira tentativa escolho raças como pintcher, vira latas, pastor alemão e são bernardo.\n\n\n\n\n\n\nHistograma base para Anova\n\n\n\n\n Como fazer\nPara reproduzir os códigos abaixo serão necessários os pacotes tidyverse , rcompanion e janitor, ggpubr, funmodeling,skimr\n\nGráfico usando rcompanion\n\n# package -----------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(rcompanion)\nlibrary(funModeling)\nlibrary(skimr)\nlibrary(ggpubr)\nlibrary(ggdist)\nlibrary(gghalves)\n\n# plot --------------------------------------------------------------------\n\n#rcompanion\nrcompanion::plotNormalHistogram(iris$Sepal.Length)\n\n\n\n\n\n\n\n\nGráfico usando funModeling\n\n#funmodeling\nmtcars %&gt;% funModeling::plot_num()\n\n\n\n\n\n\n\n\nGráfico usando skimr\n\n#skimr\ndata(ames, package = \"modeldata\")\n\names %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2930\n\n\nNumber of columns\n74\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n40\n\n\nnumeric\n34\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nMS_SubClass\n0\n1\nFALSE\n16\nOne: 1079, Two: 575, One: 287, One: 192\n\n\nMS_Zoning\n0\n1\nFALSE\n7\nRes: 2273, Res: 462, Flo: 139, Res: 27\n\n\nStreet\n0\n1\nFALSE\n2\nPav: 2918, Grv: 12\n\n\nAlley\n0\n1\nFALSE\n3\nNo_: 2732, Gra: 120, Pav: 78\n\n\nLot_Shape\n0\n1\nFALSE\n4\nReg: 1859, Sli: 979, Mod: 76, Irr: 16\n\n\nLand_Contour\n0\n1\nFALSE\n4\nLvl: 2633, HLS: 120, Bnk: 117, Low: 60\n\n\nUtilities\n0\n1\nFALSE\n3\nAll: 2927, NoS: 2, NoS: 1\n\n\nLot_Config\n0\n1\nFALSE\n5\nIns: 2140, Cor: 511, Cul: 180, FR2: 85\n\n\nLand_Slope\n0\n1\nFALSE\n3\nGtl: 2789, Mod: 125, Sev: 16\n\n\nNeighborhood\n0\n1\nFALSE\n28\nNor: 443, Col: 267, Old: 239, Edw: 194\n\n\nCondition_1\n0\n1\nFALSE\n9\nNor: 2522, Fee: 164, Art: 92, RRA: 50\n\n\nCondition_2\n0\n1\nFALSE\n8\nNor: 2900, Fee: 13, Art: 5, Pos: 4\n\n\nBldg_Type\n0\n1\nFALSE\n5\nOne: 2425, Twn: 233, Dup: 109, Twn: 101\n\n\nHouse_Style\n0\n1\nFALSE\n8\nOne: 1481, Two: 873, One: 314, SLv: 128\n\n\nOverall_Cond\n0\n1\nFALSE\n9\nAve: 1654, Abo: 533, Goo: 390, Ver: 144\n\n\nRoof_Style\n0\n1\nFALSE\n6\nGab: 2321, Hip: 551, Gam: 22, Fla: 20\n\n\nRoof_Matl\n0\n1\nFALSE\n8\nCom: 2887, Tar: 23, WdS: 9, WdS: 7\n\n\nExterior_1st\n0\n1\nFALSE\n16\nVin: 1026, Met: 450, HdB: 442, Wd : 420\n\n\nExterior_2nd\n0\n1\nFALSE\n17\nVin: 1015, Met: 447, HdB: 406, Wd : 397\n\n\nMas_Vnr_Type\n0\n1\nFALSE\n5\nNon: 1775, Brk: 880, Sto: 249, Brk: 25\n\n\nExter_Cond\n0\n1\nFALSE\n5\nTyp: 2549, Goo: 299, Fai: 67, Exc: 12\n\n\nFoundation\n0\n1\nFALSE\n6\nPCo: 1310, CBl: 1244, Brk: 311, Sla: 49\n\n\nBsmt_Cond\n0\n1\nFALSE\n6\nTyp: 2616, Goo: 122, Fai: 104, No_: 80\n\n\nBsmt_Exposure\n0\n1\nFALSE\n5\nNo: 1906, Av: 418, Gd: 284, Mn: 239\n\n\nBsmtFin_Type_1\n0\n1\nFALSE\n7\nGLQ: 859, Unf: 851, ALQ: 429, Rec: 288\n\n\nBsmtFin_Type_2\n0\n1\nFALSE\n7\nUnf: 2499, Rec: 106, LwQ: 89, No_: 81\n\n\nHeating\n0\n1\nFALSE\n6\nGas: 2885, Gas: 27, Gra: 9, Wal: 6\n\n\nHeating_QC\n0\n1\nFALSE\n5\nExc: 1495, Typ: 864, Goo: 476, Fai: 92\n\n\nCentral_Air\n0\n1\nFALSE\n2\nY: 2734, N: 196\n\n\nElectrical\n0\n1\nFALSE\n6\nSBr: 2682, Fus: 188, Fus: 50, Fus: 8\n\n\nFunctional\n0\n1\nFALSE\n8\nTyp: 2728, Min: 70, Min: 65, Mod: 35\n\n\nGarage_Type\n0\n1\nFALSE\n7\nAtt: 1731, Det: 782, Bui: 186, No_: 157\n\n\nGarage_Finish\n0\n1\nFALSE\n4\nUnf: 1231, RFn: 812, Fin: 728, No_: 159\n\n\nGarage_Cond\n0\n1\nFALSE\n6\nTyp: 2665, No_: 159, Fai: 74, Goo: 15\n\n\nPaved_Drive\n0\n1\nFALSE\n3\nPav: 2652, Dir: 216, Par: 62\n\n\nPool_QC\n0\n1\nFALSE\n5\nNo_: 2917, Exc: 4, Goo: 4, Typ: 3\n\n\nFence\n0\n1\nFALSE\n5\nNo_: 2358, Min: 330, Goo: 118, Goo: 112\n\n\nMisc_Feature\n0\n1\nFALSE\n6\nNon: 2824, She: 95, Gar: 5, Oth: 4\n\n\nSale_Type\n0\n1\nFALSE\n10\nWD : 2536, New: 239, COD: 87, Con: 26\n\n\nSale_Condition\n0\n1\nFALSE\n6\nNor: 2413, Par: 245, Abn: 190, Fam: 46\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nLot_Frontage\n0\n1\n57.65\n33.50\n0.00\n43.00\n63.00\n78.00\n313.00\n▇▇▁▁▁\n\n\nLot_Area\n0\n1\n10147.92\n7880.02\n1300.00\n7440.25\n9436.50\n11555.25\n215245.00\n▇▁▁▁▁\n\n\nYear_Built\n0\n1\n1971.36\n30.25\n1872.00\n1954.00\n1973.00\n2001.00\n2010.00\n▁▂▃▆▇\n\n\nYear_Remod_Add\n0\n1\n1984.27\n20.86\n1950.00\n1965.00\n1993.00\n2004.00\n2010.00\n▅▂▂▃▇\n\n\nMas_Vnr_Area\n0\n1\n101.10\n178.63\n0.00\n0.00\n0.00\n162.75\n1600.00\n▇▁▁▁▁\n\n\nBsmtFin_SF_1\n0\n1\n4.18\n2.23\n0.00\n3.00\n3.00\n7.00\n7.00\n▃▂▇▁▇\n\n\nBsmtFin_SF_2\n0\n1\n49.71\n169.14\n0.00\n0.00\n0.00\n0.00\n1526.00\n▇▁▁▁▁\n\n\nBsmt_Unf_SF\n0\n1\n559.07\n439.54\n0.00\n219.00\n465.50\n801.75\n2336.00\n▇▅▂▁▁\n\n\nTotal_Bsmt_SF\n0\n1\n1051.26\n440.97\n0.00\n793.00\n990.00\n1301.50\n6110.00\n▇▃▁▁▁\n\n\nFirst_Flr_SF\n0\n1\n1159.56\n391.89\n334.00\n876.25\n1084.00\n1384.00\n5095.00\n▇▃▁▁▁\n\n\nSecond_Flr_SF\n0\n1\n335.46\n428.40\n0.00\n0.00\n0.00\n703.75\n2065.00\n▇▃▂▁▁\n\n\nGr_Liv_Area\n0\n1\n1499.69\n505.51\n334.00\n1126.00\n1442.00\n1742.75\n5642.00\n▇▇▁▁▁\n\n\nBsmt_Full_Bath\n0\n1\n0.43\n0.52\n0.00\n0.00\n0.00\n1.00\n3.00\n▇▆▁▁▁\n\n\nBsmt_Half_Bath\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0.00\n2.00\n▇▁▁▁▁\n\n\nFull_Bath\n0\n1\n1.57\n0.55\n0.00\n1.00\n2.00\n2.00\n4.00\n▁▇▇▁▁\n\n\nHalf_Bath\n0\n1\n0.38\n0.50\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▁\n\n\nBedroom_AbvGr\n0\n1\n2.85\n0.83\n0.00\n2.00\n3.00\n3.00\n8.00\n▁▇▂▁▁\n\n\nKitchen_AbvGr\n0\n1\n1.04\n0.21\n0.00\n1.00\n1.00\n1.00\n3.00\n▁▇▁▁▁\n\n\nTotRms_AbvGrd\n0\n1\n6.44\n1.57\n2.00\n5.00\n6.00\n7.00\n15.00\n▁▇▂▁▁\n\n\nFireplaces\n0\n1\n0.60\n0.65\n0.00\n0.00\n1.00\n1.00\n4.00\n▇▇▁▁▁\n\n\nGarage_Cars\n0\n1\n1.77\n0.76\n0.00\n1.00\n2.00\n2.00\n5.00\n▅▇▂▁▁\n\n\nGarage_Area\n0\n1\n472.66\n215.19\n0.00\n320.00\n480.00\n576.00\n1488.00\n▃▇▃▁▁\n\n\nWood_Deck_SF\n0\n1\n93.75\n126.36\n0.00\n0.00\n0.00\n168.00\n1424.00\n▇▁▁▁▁\n\n\nOpen_Porch_SF\n0\n1\n47.53\n67.48\n0.00\n0.00\n27.00\n70.00\n742.00\n▇▁▁▁▁\n\n\nEnclosed_Porch\n0\n1\n23.01\n64.14\n0.00\n0.00\n0.00\n0.00\n1012.00\n▇▁▁▁▁\n\n\nThree_season_porch\n0\n1\n2.59\n25.14\n0.00\n0.00\n0.00\n0.00\n508.00\n▇▁▁▁▁\n\n\nScreen_Porch\n0\n1\n16.00\n56.09\n0.00\n0.00\n0.00\n0.00\n576.00\n▇▁▁▁▁\n\n\nPool_Area\n0\n1\n2.24\n35.60\n0.00\n0.00\n0.00\n0.00\n800.00\n▇▁▁▁▁\n\n\nMisc_Val\n0\n1\n50.64\n566.34\n0.00\n0.00\n0.00\n0.00\n17000.00\n▇▁▁▁▁\n\n\nMo_Sold\n0\n1\n6.22\n2.71\n1.00\n4.00\n6.00\n8.00\n12.00\n▅▆▇▃▃\n\n\nYear_Sold\n0\n1\n2007.79\n1.32\n2006.00\n2007.00\n2008.00\n2009.00\n2010.00\n▇▇▇▇▃\n\n\nSale_Price\n0\n1\n180796.06\n79886.69\n12789.00\n129500.00\n160000.00\n213500.00\n755000.00\n▇▇▁▁▁\n\n\nLongitude\n0\n1\n-93.64\n0.03\n-93.69\n-93.66\n-93.64\n-93.62\n-93.58\n▅▅▇▆▁\n\n\nLatitude\n0\n1\n42.03\n0.02\n41.99\n42.02\n42.03\n42.05\n42.06\n▂▂▇▇▇\n\n\n\n\n\nGráfico usando ggpubr\n\n#ggpubr\ndata(pistonrings, package = \"qcc\")\n\npistonrings %&gt;% \n  ggpubr::gghistogram(x = \"diameter\", \n                      fill = \"trial\",\n                      add = \"mean\",\n                      rug = TRUE)\n\n\n\n\n\n\n\n\nGráfico usando base raincloud\n\n#raincloud\names &lt;- ames %&gt;% janitor::clean_names()\n\names %&gt;% \n  mutate(sale_price = log10(sale_price)) %&gt;% \n  ggplot(aes(x = heating_qc,\n             y = sale_price))+\n  ggdist::stat_halfeye()\n\n\n\n\n\n\n\n\n\n\n Pra onde vai quem é o cliente\n\nVai para a etapa de pre processamento ou ajuste de coleta de dados.\n\n\n\n Qual o resultado\n\nAperfeiçoar a avalição preliminar dos dados através da análise de distribuição.\nFacilitar o reconhecimento de determinados padrões que poderão influenciar nas decisões.\nMelhorar o entendimento sobre a natureza dos dados."
  },
  {
    "objectID": "posts/importar-visualizar/importar_visualizar.html",
    "href": "posts/importar-visualizar/importar_visualizar.html",
    "title": "Como importar e visualizar dados - Titanic dataset parte1",
    "section": "",
    "text": "Vídeo tema para este post Como importar e visualizar dados - Titanic dataset parte1\n\n\n\n\n O que é importação e visualização de dados\n\nÉ a etapa onde é feita extração de dados e avaliação se os mesmos podem contribuir para a solução do problema a ser resolvido.\nA visualização é uma forma intuitiva de traçar relações possíveis entre os dados e o desfecho permitindo a geração de questões e levantamento de hipóteses.\nGeralmente estão atrelados a uma métrica de sucesso e é feito após a etapa P do PDCA , PPDAC ou outro método de análise de fenõmeno e causa raíz utilizado.\n\n\n\nProcesso de projeto kaizen\n\n\n\n\n\n Qual o objetivo\n\nDisponibilizar recursos revelantes para solução dos problemas\nFiltrar dados relevantes dos não relevantes, entender a natureza dos dados se são realmente ruídos ou carecem de representação.\nAvaliar existencia de shadow stats ou se haverá dificuldade na coleta. Por exemplo: o dado pode ser muito relevante mas difícil de ser coletado ou pode haver muita instabilidade e ambiguidade ou até mesmo usar critérios diferentes entre diferentes fontes.\nGerar novas questões e hipóteses que poderão utilizar novos dados ou um resultado de interação entre os existentes.\n\n\n\n De onde vem a demanda\n\nA extração e visualização dos dados vem da necessidade de encontrar variáveis que expliquem o fenômeno e ajudem a prever eventos futuros.\nMelhorar e estimular a geração de ideias através da interação entre os envolvidos.\n\n\n\n Como fazer\nPara reproduzir os códigos abaixo serão necessários os pacotes tidyverse , tidymodels e janitor, ggpubr, funmodeling, ggalluvial e visdat\n\nO dados podem ser baixados no Kaggle titanic dataset\n\n# pacotes           --------------------------------------------------------\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(ggpubr)\nlibrary(funModeling)\nlibrary(ggalluvial)\nlibrary(visdat)\n\n# dados             --------------------------------------------------------\n#importar os dados do kaggle e salvar em  diretorio padrão\n\n# importar dados e padronizar colunas\ntrain_titanic &lt;- \n    read.csv(\"train.csv\", na.strings = c(\"\",\" \")) %&gt;% \n    clean_names() %&gt;% \n    mutate(is_train = TRUE) \n\n#importar dados de teste\ntest_titanic &lt;- \n  read.csv(\"test.csv\", na.strings = c(\"\",\" \")) %&gt;% \n  clean_names() %&gt;% \n  mutate(survived = NA, is_train = FALSE) \n\n#juntar os conjuntos\nsplit_titanic &lt;- bind_rows(train_titanic, test_titanic)\n\n#check nas caracteristicas dos dados\nsplit_titanic %&gt;% funModeling::df_status()\n\n       variable q_zeros p_zeros q_na  p_na q_inf p_inf      type unique\n1  passenger_id       0    0.00    0  0.00     0     0   integer   1309\n2      survived     549   41.94  418 31.93     0     0   integer      2\n3        pclass       0    0.00    0  0.00     0     0   integer      3\n4          name       0    0.00    0  0.00     0     0 character   1307\n5           sex       0    0.00    0  0.00     0     0 character      2\n6           age       0    0.00  263 20.09     0     0   numeric     98\n7        sib_sp     891   68.07    0  0.00     0     0   integer      7\n8         parch    1002   76.55    0  0.00     0     0   integer      8\n9        ticket       0    0.00    0  0.00     0     0 character    929\n10         fare      17    1.30    1  0.08     0     0   numeric    281\n11        cabin       0    0.00 1014 77.46     0     0 character    186\n12     embarked       0    0.00    2  0.15     0     0 character      3\n13     is_train     418   31.93    0  0.00     0     0   logical      2\n\n\nComo o nosso conjunto de dados é pequeno(tem poucas variáveis) é possível usar um gráfico para visualizar os tipos de dados e as características.\n\nsplit_titanic %&gt;% visdat::vis_dat()\n\n\n\n\nGráfico Visdat - Características das variáveis\n\n\n\n\n\n\n\n\n\n\nComo faço pra visualizar dados faltantes quando o conjunto for grande ?\n\n\n\n\n\nSugestão É possível usar o pacote naniar com a função naniar::miss_var_summary()\n\n\n\nComo resultado da análise podemos verificar a necessidade de algumas transformações nos dados. Anotamos tudo para alterarmos posteriormente mas seguimos com a análise exploratória por enquanto.\n\nAlterar o tipo de variável de sex, embarked, survived, pclass = factor\nInserir dados faltantes em embarked = moda, age e fare = usando knn\n\nVamos fazer uma correlação para avaliar os padrões existentes. Mas note. Para fazer correlação, os dados precisam ser numéricos por isso usamos a função select_if(is.numeric) . Pelo padrão apresentado é possível verificar a existencia de dados categóricos eles geralmente ficam espaçados no gráfico e eles precisam ser transformados pra factor posteriormente e que já anotamos e comunicamos.\n\n# correlacao\ntrain_titanic %&gt;% \n  select_if(is.numeric) %&gt;% \n  GGally::ggscatmat(color = \"survived\", corMethod = \"spearman\")+\n  theme_pubclean()\n\n\n\n\nGráfico de correlação de Spearman\n\n\n\n\nSerá que a idade importa ?\n\n# age - será que a idade importa ? \ntrain_titanic %&gt;% \n  ggplot(aes(x=age, fill = factor(survived)))+\n  geom_density(alpha =0.5)\n\n\n\n\n\n\n\n\nSerá que o sexo importa ?\n\n# age, sex and class - será que o sexo importa ?\ntrain_titanic %&gt;% \n  ggplot(aes(x=pclass, fill= factor(survived)))+\n  geom_bar(stat = \"count\")+\n  facet_grid(~sex)\n\n\n\n\n\n\n\n\nVamos juntar as variáveis age+sex+pclass e analisar sob outro ponto de vista\n\n# age sex and class - sob outro ponto de vista\ntrain_titanic %&gt;% \n  ggplot(aes(x=age, y=sex))+\n  geom_jitter(aes(color = factor(survived)))+\n  facet_wrap(~pclass)\n\n\n\n\n\n\n\n\nNovamente vamos analisar sob outro ponto de vista\n\n# sex, class e survived \ntrain_titanic %&gt;% \n  group_by(sex, survived, pclass) %&gt;% \n  summarise(qtd = n()) %&gt;% \n  ggplot(aes(axis1=sex, axis2=pclass, axis3 = survived, y=qtd, fill= sex))+\n  geom_alluvium()+\n  geom_stratum()+\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum)))+\n  scale_x_discrete(limits = c(\"sex\", \"pclass\",\"survived\"))\n\n\n\n\n\n\n\n\nSerá que a variável tarifa está bem representada ?\n\n# fare - será que fare é uma variável válida ?\ntrain_titanic %&gt;% \n  mutate(fare = fare) %&gt;% \n  ggplot(aes(x=fare, y=pclass))+\n  geom_jitter(aes(color = factor(survived)))\n\n\n\n\n\n\n\n\nSera que o tamanho da familia importa e o dado está bem representado ?\n\n# parch sib_sp - será que o tamanho da familia importa ?\ntrain_titanic %&gt;% \n  mutate(family_size = parch + sib_sp +1) %&gt;%\n  ggboxplot(x=\"survived\", \n            y=\"family_size\", \n            fill= \"survived\",\n            palette = \"uchicago\")+\n  stat_compare_means()\n\n\n\n\n\n\n\n\nSerá que existe algum padrão no nome que possa ser extraído e que ajude a explicar se impacta na sobrevivência ?\n\n# title\ntrain_titanic %&gt;% \n  mutate(title = str_extract(name, \"[A-z]*\\\\.\")) %&gt;% \n  tabyl(title) %&gt;% \n  adorn_pct_formatting() %&gt;% \n  arrange(desc(n))\n\n     title   n percent\n       Mr. 517   58.0%\n     Miss. 182   20.4%\n      Mrs. 125   14.0%\n   Master.  40    4.5%\n       Dr.   7    0.8%\n      Rev.   6    0.7%\n      Col.   2    0.2%\n    Major.   2    0.2%\n     Mlle.   2    0.2%\n     Capt.   1    0.1%\n Countess.   1    0.1%\n      Don.   1    0.1%\n Jonkheer.   1    0.1%\n     Lady.   1    0.1%\n      Mme.   1    0.1%\n       Ms.   1    0.1%\n      Sir.   1    0.1%\n\n\n\n\n Pra onde vai quem é o cliente\n\nA próxima etapa é comunicar os resultados junto aos membros do projeto sobre as relações encontradas permitindo a interção e levantamento de novas necessidades ou hipóteses.\nPosteriormente, vem as etapas de pré processamento, modelo, validação e submissão.\n\n\n\n Qual o resultado\n\nAperfeiçoar a técnica de observação e representação de dados.\nFacilitar a comunicação com os envolvidos no projeto.\nMelhorar e estimular a geração de ideias através da interação entre os membros do projeto."
  },
  {
    "objectID": "posts/neural-net-nnet/neuralnet.html",
    "href": "posts/neural-net-nnet/neuralnet.html",
    "title": "Modelo preditivo usando com rede neural",
    "section": "",
    "text": "Vídeo tema para este post Redes neurais artificiais no R com nnet package\n\n\n\n\n O que é um modelo de rede neural\n\nRede neural é uma técnica que usa nós interconectados ou neuronios em uma estrutura de camadas com o objetivo de resolver problemas de previsão.\n\nRede neural regressãoRede neural Classificação\n\n\n\n\n\nRede Neural - Regressão\n\n\n\n\n\n\n\nRede Neural - Classificação\n\n\n\n\n\n\n\n\n Qual o objetivo\n\nA técnica é muito utilizada na solução de problemas de classificação onde o resultado(desfecho) é uma classe (e.g. gato, cachorro, cavalo).\nNeste exemplo, nós usaremos como exemplo uma linha de produção com sensores que registram características como largura e comprimento da pétala e sepala dos três tipos de flores iris: setosa, versicolor e virgínica.\nNosso objetivo é fazer um modelo de previsão o qual com base nessas características, consiga classificar cada tipo de flor e separá-las corretamente.\n\n\n\n De onde vem a demanda\n\nVem do uso de sistemas que utilizam visão computacional, processamento de linguagem natural em seu funcionamento como por exemplo veículos semi-autonomos, linhas de produção de veículos, controle de qualidade de autopeças entre outros.\nA primeira rede neural artificial foi implementada em 1958 por Frank Rosenblat e se chamava Perceptron e naquela época já se falava que era um embrião de um computador eletronico que seria capaz de andar, falar, ver, escrever e se reproduzir. Materia New York Times\n\n\n\n Como fazer\nNeste exemplo usaremos os pacotes tidyverse, janitor, tidymodels, nnet e neuralnettools\n\n# packages ----------------------------------------------------\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(nnet)\nlibrary(NeuralNetTools)\n\n# data --------------------------------------------------------\ndata_iris &lt;- iris %&gt;% janitor::clean_names()\n\nsplit_iris &lt;- initial_split(data_iris, strata = species)\ntrain_iris &lt;- training(split_iris)\ntest_iris  &lt;- testing(split_iris)\n\n# model -------------------------------------------------------\nmdl_fit_nn_iris &lt;- nnet(species ~., \n                        data = train_iris, \n                        size = 5, \n                        decay = 0.01, \n                        maxit = 500)\n\n# weights:  43\ninitial  value 142.466668 \niter  10 value 14.781617\niter  20 value 9.929478\niter  30 value 8.291768\niter  40 value 7.849535\niter  50 value 7.765939\niter  60 value 7.732812\niter  70 value 7.696247\niter  80 value 7.684596\niter  90 value 7.675855\niter 100 value 7.669159\niter 110 value 7.662754\niter 120 value 7.658608\niter 130 value 7.657329\niter 140 value 7.657058\niter 150 value 7.657015\nfinal  value 7.657012 \nconverged\n\n\nVamos fazer uma previsão usando o modelo que construímos e colocar o resultado dentro do dataframe com os dados de teste para podermos comparar valores previstos e realizados.\n\n# results -----------------------------------------------------\nmdl_fit_nn_iris %&gt;% \n  predict(test_iris,type = \"class\") %&gt;%\n  bind_cols(test_iris)\n\nNew names:\n• `` -&gt; `...1`\n\n\n         ...1 sepal_length sepal_width petal_length petal_width    species\n1      setosa          4.9         3.0          1.4         0.2     setosa\n2      setosa          5.0         3.4          1.5         0.2     setosa\n3      setosa          5.4         3.7          1.5         0.2     setosa\n4      setosa          5.8         4.0          1.2         0.2     setosa\n5      setosa          5.1         3.5          1.4         0.3     setosa\n6      setosa          5.7         3.8          1.7         0.3     setosa\n7      setosa          5.4         3.4          1.7         0.2     setosa\n8      setosa          5.0         3.4          1.6         0.4     setosa\n9      setosa          4.9         3.6          1.4         0.1     setosa\n10     setosa          5.1         3.4          1.5         0.2     setosa\n11     setosa          5.0         3.5          1.6         0.6     setosa\n12     setosa          5.1         3.8          1.6         0.2     setosa\n13     setosa          5.3         3.7          1.5         0.2     setosa\n14 versicolor          5.5         2.3          4.0         1.3 versicolor\n15 versicolor          6.3         3.3          4.7         1.6 versicolor\n16 versicolor          4.9         2.4          3.3         1.0 versicolor\n17 versicolor          6.2         2.2          4.5         1.5 versicolor\n18 versicolor          5.6         2.5          3.9         1.1 versicolor\n19 versicolor          6.7         3.0          5.0         1.7 versicolor\n20 versicolor          5.7         2.6          3.5         1.0 versicolor\n21 versicolor          6.0         3.4          4.5         1.6 versicolor\n22 versicolor          6.7         3.1          4.7         1.5 versicolor\n23 versicolor          5.5         2.6          4.4         1.2 versicolor\n24 versicolor          5.6         2.7          4.2         1.3 versicolor\n25 versicolor          5.7         3.0          4.2         1.2 versicolor\n26 versicolor          5.7         2.8          4.1         1.3 versicolor\n27  virginica          7.1         3.0          5.9         2.1  virginica\n28  virginica          4.9         2.5          4.5         1.7  virginica\n29  virginica          5.7         2.5          5.0         2.0  virginica\n30  virginica          5.8         2.8          5.1         2.4  virginica\n31  virginica          6.4         3.2          5.3         2.3  virginica\n32  virginica          7.7         2.8          6.7         2.0  virginica\n33  virginica          6.2         2.8          4.8         1.8  virginica\n34  virginica          7.2         3.0          5.8         1.6  virginica\n35  virginica          7.9         3.8          6.4         2.0  virginica\n36 versicolor          6.1         2.6          5.6         1.4  virginica\n37  virginica          6.4         3.1          5.5         1.8  virginica\n38  virginica          5.8         2.7          5.1         1.9  virginica\n39  virginica          6.3         2.5          5.0         1.9  virginica\n\n\nComo já temos o resultados previstos e o real, podemos fazer uma matriz de confusão para facilitar a visualização dos resultados e analisar o desempenho inicial do modelo.\n\n#confusion matrix\ntable(test_iris$species,\n      predict(mdl_fit_nn_iris,\n              newdata = test_iris, \n              type = \"class\"))\n\n            \n             setosa versicolor virginica\n  setosa         13          0         0\n  versicolor      0         13         0\n  virginica       0          1        12\n\n\nComo ficou o resultado da previsão ?\n\n#predict classes\npredict(mdl_fit_nn_iris, test_iris, type = \"class\")\n\n [1] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n [6] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[11] \"setosa\"     \"setosa\"     \"setosa\"     \"versicolor\" \"versicolor\"\n[16] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[21] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[26] \"versicolor\" \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n[31] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n[36] \"versicolor\" \"virginica\"  \"virginica\"  \"virginica\" \n\n\nPlotando o modelo que acabamos de fazer\n\n#plotando o gráfico\nNeuralNetTools::plotnet(mdl_fit_nn_iris,pad_x=0.55, circle_col = \"grey90\")\n\n\n\n\n\n\n\n\n\n\n Pra onde vai quem é o cliente\n\nUm modelos de previsão de classificação pode ser embarcado dentro do sistema de produção.\nO modelo que fizemos é bastante simples e serve apenas como base para aprendizado, como próximo passo o uso de validação cruzada (cross validation) pode ser uma excelente opção para complementar o que aprendemos aqui.\n\n\n\n Qual o resultado\n\nAperfeiçoar as técnicas de contrução de modelos e solução de problemas de classificação para que estes possam auxiliar na produtividade da industria de flores que usamos como exemplo.\nFacilitar o processo de classificação.\nMelhorar o processo de produção, qualidade do produto, satisfação do consumidor e competitividade do negócio."
  },
  {
    "objectID": "posts/pattern-recognition/pattern_recognition.html",
    "href": "posts/pattern-recognition/pattern_recognition.html",
    "title": "Como Identificar padrões em dados - gráfico de dispersão",
    "section": "",
    "text": "Vídeo tema para este post em Como Identificar padrões em dados - gráfico de dispersão\n\n\n\n\n O que são padrões em dados\n\nPadrões são fenômenos que se repetem de forma regular com base em alguma regra ou em condições definidas.\nPadrões nos permite fazer comparações e com isso deu origem a descobertas e invenções as quais são resultados da habilidade humanda de reconhecer padrões.\nReconhecer padrões requer repetição da experiência, e compreender os padrões é um dos fundamentos do pensamento matemático e resolução de problemas.\nExemplo de padrões - número, som, imagem, cores, plantas, linguagem.\n\n\n\n\nTabuleta de barro micenica de 1200 antes de Cristo com informações sobre a distrubição de couro bovino, suíno e veado aos sapateiros - PY Ub 1318\n\n\n\n\n Qual o objetivo\n\nPermitir que possamos fazer previsões e ou explicar melhor o fenômeno.\nEncontrar dados relevantes para que consigamos replicar esses fenomenos para construir coisas que melhorem e simplifiquem nossa vida\n\n\n\n De onde vem a demanda\n\nDa necessidade de investigar as características com visão ampla sobre diversos pontos de vista PDCA.\nNo exemplo utilizado a)Quem eram as vítimas ? b)Em que período ele as intoxicava ? c)Em qual período de tempo ?\n\n\n\n\nPDCA - Etapa 2\n\n\n\n\n Como fazer\nPara reproduzir os códigos abaixo serão necessários os pacotes tidyverse , ggExtra e janitor\n\nO dados podem ser baixados no github de David Spiegelhalter\n\n# packages  --------------------------------------------------------------------\n\nlibrary(tidyverse)\nlibrary(ggExtra)\nlibrary(janitor)\nlibrary(scales)\n\n# data  -------------------------------------------------------------------\ndata_crime &lt;- \n  read.csv(\"https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/00-1-age-and-year-of-deathofharold-shipmans-victims/00-1-shipman-confirmed-victims-x.csv\") %&gt;% \n  janitor::clean_names()\n\ncrime_time &lt;- \n  read.csv(\"https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/00-2-shipman-times/00-2-shipman-times-x.csv\") %&gt;% \n  janitor::clean_names()\n\n# plot    -------------------------------------------------------------------\n#diagrama de dispersao\nplot &lt;- \ndata_crime %&gt;% \n  ggplot(aes(x = fractional_death_year, y = age, color = gender2))+\n  geom_point()+\n  labs(title = \"vitimas de shipman\",\n       x = \"ano\",\n       y = \"idade\")+\n  theme(legend.title = element_blank(), legend.position = c(.125,1.15))\n\nggExtra::ggMarginal(plot, type = \"histogram\")\n\n\n\n\n\n\n\n#linha\ncrime_time %&gt;% \n  ggplot(aes(x= hour, y))+\n  geom_line(aes(y = shipman, col = \"Shipman\"))+\n  geom_line(aes(y = comparison, col = \"Outros\"))+\n  scale_y_continuous(limits = c(0,15), labels = label_percent(scale=1))\n\n\n\n\n\n\n\n\n\n\n Pra onde vai quem é o cliente\n\nA próxima etapa pra quem usa o PDCA é a busca pelas causas fundamentais do problema.\n\n\n\n\nPDCA - Etapa 3\n\n\n\n\n Qual o resultado\n\nAperfeiçoar as técnicas de análise de fenômeno encurtando o tempo para explicação ou solução de algum problema\nFacilitar a replicação do fenômeno e consequentemente a comunicação.\nMelhorar as chances de sucesso na solução de problemas ou fazer qualquer coisa bem.\n\n\n\n Referência\n\n\n\nA arte da estatística: Como aprender a partir de dados por David Spiegelhalter, George Schlesinger"
  },
  {
    "objectID": "posts/table_stats/table_stats.html",
    "href": "posts/table_stats/table_stats.html",
    "title": "Como construir tabela para artigo técnico - gtsummary",
    "section": "",
    "text": "O que é uma tabela de dados\n\nTabela é um meio eficiente de representação e comunicação de dados tendo como características principais linhas, colunas, sumarizações e comparativos.\nDevido a versatilidade e capacidade de informações, é comum o uso em publicações técnicas e científicas.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Tabuleta de barro de 539 antes de Cristo de Nippur Mesopotâmia atual Iraque, considerada uma das primeiras planilhas\n\n\n\n\n\n\n\n\n\n\n\n(b) Resultado financeiro de uma empresa DRE\n\n\n\n\n\n\n\n\n\n\n\n(c) Modelo de tabela nutricional de lasanha congelada- FDA\n\n\n\n\n\n\n\nFigure 1: Exemplo de tabelas: Créditos para Penn Museum, factorialmap (DRE = P&L in US), FDA Food and Drug Administration\n\n\n\n\n\n Qual o objetivo\n\nRepresentar de forma compacta informações importantes que podem ser difíceis de expressar em texto ou gráficos.\nPermitir a comparação entre valores, resumir ou definir conceitos, termos ou outros detalhes de um estudo.\nPermitir que o leitor veja rapidamente os resultados de dados complexos os quais são organizados e descritos adequadamente no texto Slutsky (2014).\nTrazer clareza nos resultados apresentados.\n\n\n\n De onde vem a demanda\n\nDa necessidade de registrar, computar e comunicar dados.\nNecessidade de mostrar muitos valores em um pequeno espaço.\nNecessidade de comparar e contrastar valores de dados com várias características ou variáveis compartilhadas.\nPara mostrar a presença ou ausência de características específicas.\n\n\n\n Como fazer\nPara reproduzir os códigos abaixo serão necessários os pacotes tidyverse, janitor, gtsummary, gtExtras, gt.\nO dados usados iris, mtcars, trial, os quais estão nos pacotes que serão carregados não sendo necessário baixá-los. Já o dado corn pode ser baixado em Kniss AR, Streibig JC (2018) Statistical Analysis of Agricultural Experiments using R\n Entender bem a estrutura dos dados pode facilitar o trabalho de construção de tabelas .\n\n\n\n\n\nflowchart LR\n    A[Variável] --&gt;B(Numerica)\n    A --&gt;C(Categórica)\n    B --&gt; D{Número \\n inteiro?}\n    D --&gt;|Sim| E(Discreto)\n    D --&gt;|Não| F(Contínuo)\n    C --&gt; G{Quantas \\n categorias?}\n    G --&gt;|Duas|H(Binária\\Dicotômica)\n    G --&gt;|Três ou mais|I{Segue \\n uma ordem?}\n    I --&gt;|Sim|J(Ordinal)\n    I --&gt;|Não|L(Norminal)\n\n\n\n\n\n\n\n# package          -----------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(gtExtras)\nlibrary(gt)\nlibrary(survival)\n\n# data             --------------------------------------------------------------------\ndata_mtcars &lt;- mtcars %&gt;% janitor::clean_names()\ndata_iris   &lt;- iris %&gt;% janitor::clean_names()\ndata_trial  &lt;- trial\ndata_corn   &lt;- read.csv(\"http://rstats4ag.org/data/irrigcorn.csv\") %&gt;% \n  janitor::clean_names()\n\n\ndata_mtcars %&gt;% \n  select(mpg, cyl, wt, vs) %&gt;% \n  tbl_summary(by= vs,\n              label = c(mpg ~ \"Milhas/galão\",\n                        cyl ~ \"Cilindros\",\n                        wt ~ \"Peso\")) %&gt;% \n  modify_header(label = \"**Variáveis**\",\n                stat_1 = \"V {n}\",\n                stat_2 = \"S {n}\") %&gt;% \n  modify_spanning_header(c(\"stat_1\",\"stat_2\") ~ \"**Type of Engine**\") %&gt;% \n  modify_caption(\"**Figura1. Consumo de gasolina por tipo de motor**\") %&gt;% \n  add_difference() %&gt;% \n  bold_p(t = 0.05) %&gt;% \n  bold_labels() %&gt;% \n  modify_caption(\"&lt;div style='text-align:left;\n                 font-weight: bold;\n                 color:grey'&gt;Figura1. Consumo de gasolina por tipo de motor &lt;/div&gt;\")\n\n\n\n\n\n\nFigura1. Consumo de gasolina por tipo de motor\n\n\n\n\n\n\n\n\n\n\nVariáveis\nType of Engine\nDifference2\n95% CI2,3\np-value2\n\n\nV 181\nS 141\n\n\n\n\nMilhas/galão\n15.7 (14.8, 19.1)\n22.8 (21.4, 29.6)\n-7.9\n-11, -4.4\n&lt;0.001\n\n\nCilindros\n\n\n\n\n2.8\n1.8, 3.8\n\n\n\n\n    4\n1 (5.6%)\n10 (71%)\n\n\n\n\n\n\n\n\n    6\n3 (17%)\n4 (29%)\n\n\n\n\n\n\n\n\n    8\n14 (78%)\n0 (0%)\n\n\n\n\n\n\n\n\nPeso\n3.57 (3.24, 3.84)\n2.62 (2.00, 3.21)\n1.1\n0.49, 1.7\n&lt;0.001\n\n\n\n1 Median (IQR); n (%)\n\n\n2 Welch Two Sample t-test; Standardized Mean Difference\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\nlibrary(survival)\n\n\ntbl_fit_logreg_trial &lt;- \n  glm(response~ trt + grade + age, data = data_trial, family = binomial(link=\"logit\")) %&gt;% \n  tbl_regression(exponentiate= TRUE)\n\ntbl_fit_coxph_trial &lt;- \ncoxph(Surv(ttdeath, death)~ trt + grade + age, data = data_trial) %&gt;% \n  tbl_regression(exponentiate = TRUE)\n\ntbl_merge(tbls = list(tbl_fit_logreg_trial, tbl_fit_coxph_trial),\n          tab_spanner = c(\"**Log reg**\", \"**Coxph**\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nLog reg\nCoxph\n\n\nOR1\n95% CI1\np-value\nHR1\n95% CI1\np-value\n\n\n\n\nChemotherapy Treatment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Drug A\n—\n—\n\n\n—\n—\n\n\n\n\n    Drug B\n1.13\n0.60, 2.13\n0.7\n1.30\n0.88, 1.92\n0.2\n\n\nGrade\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    I\n—\n—\n\n\n—\n—\n\n\n\n\n    II\n0.85\n0.39, 1.85\n0.7\n1.21\n0.73, 1.99\n0.5\n\n\n    III\n1.01\n0.47, 2.15\n&gt;0.9\n1.79\n1.12, 2.86\n0.014\n\n\nAge\n1.02\n1.00, 1.04\n0.10\n1.01\n0.99, 1.02\n0.3\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval, HR = Hazard Ratio\n\n\n\n\n\n\n\n\n\n\ndata_corn %&gt;% \n  select(irrig, yield_tonha, yield_bu_a) %&gt;% \n  tbl_summary(by = irrig,\n              label = list(yield_tonha ~ \"Tons/hec\",\n                           yield_bu_a ~ \"Bushel Acre\")) %&gt;% \n  add_difference() %&gt;% \n  modify_header(label = \"**Yield**\") %&gt;% \n  modify_spanning_header(all_stat_cols() ~ \"**Irrigation type** {N} \")\n\n\n\n\n\n\n\n\nYield\nIrrigation type 96\nDifference2\n95% CI2,3\np-value2\n\n\nFull, N = 481\nLimited, N = 481\n\n\n\n\nTons/hec\n11.42 (10.67, 12.08)\n10.14 (9.40, 11.41)\n1.1\n0.63, 1.6\n&lt;0.001\n\n\nBushel Acre\n182 (170, 193)\n162 (150, 182)\n18\n10, 26\n&lt;0.001\n\n\n\n1 Median (IQR)\n\n\n2 Welch Two Sample t-test\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\nmtcars %&gt;%\n  slice_sample(n=8) %&gt;% \n  gt() %&gt;% \n  gt_theme_guardian() %&gt;% \n  data_color(columns = mpg:hp, colors = c(\"white\",\"red\")) %&gt;% \n  gt_highlight_rows(rows = 2, font_weight = \"bold\")\n\nWarning: Since gt v0.9.0, the `colors` argument has been deprecated.\n• Please use the `palette` argument to define a color palette.\nThis warning is displayed once every 8 hours.\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n14.7\n8\n440.0\n230\n3.23\n5.345\n17.42\n0\n0\n3\n4\n\n\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n10.4\n8\n472.0\n205\n2.93\n5.250\n17.98\n0\n0\n3\n4\n\n\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\n\n\n\n\n\nmtcars %&gt;% \n  group_by(cyl) %&gt;% \n  summarise(mpg_data  = list(mpg), .groups = \"drop\") %&gt;% \n  gt() %&gt;% \n  gt_plt_sparkline(mpg_data)\n\n\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg_data\n\n\n\n\n4\n\n\n\n   21.4\n\n\n\n6\n\n\n\n   19.7\n\n\n\n8\n\n\n\n   15.0\n\n\n\n\n\n\n\n\nmtcars %&gt;% \n  group_by(cyl) %&gt;% \n  summarise(mpg_dat = list(mpg)) %&gt;% \n  gt() %&gt;% \n  gt_plt_dist(mpg_dat)\n\n\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg_dat\n\n\n\n\n4\n\n\n\n   \n\n\n\n6\n\n\n\n   \n\n\n\n8\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n Pra onde vai quem é o cliente\n\nEtapas de analise de resultados, validação, remodelagem ou comunicação.\nInclusão em documento técnico reproduzível.\nComunicação de descobertas ou resultados de trabalhos técnicos.\nDocumentação ou treinamento de pessoal.\n\n\n\n Qual o resultado\n\nAperfeiçoar as técnicas de representação, calculo e comunicação de dados e estudos técnicos\nFacilitar o entendimento das informações contidas nos documentos técnicos poupando tempo e dinheiro, permitindo a replicação e experimentos e consequentemente busca por melhoria.\nFacilitar o reconhecimento de registros, estudos, descobertas em pesquisas futuras\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Registro de entrega de alimentos em diferentes regiões como Delta do Nilo. A receita em vermelho, a despesa em preto.\n\n\n\n\n\n\n\n\n\n\n\n(b) Arquivo de Merer registro de atividade de 200 homens durante a construção da pirâmide de Gizé\n\n\n\n\n\n\n\nFigure 2: Papiros escritos em Hierático em 2550 antes de Cristo final do reinado de Khufu(Queops) e usados para registrar atividades durante a construção da pirâmide de Khufu. Créditos: Museu do Cairo, Egito.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nSlutsky, David. 2014. “The Use of Tables.” Journal of Wrist Surgery 03 (04): 219–19. https://doi.org/10.1055/s-0034-1395165."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Motivação para o blog",
    "section": "",
    "text": "Vídeo tema para este post Transição demográfcias Brasil e Japão\n\n\n\n\n Por que este blog e o canal no YouTube ?\nEm 2040 o Brasil terá uma população mais urbana e mais madura. A necessidade de aumento de produtividade no país estará em rota de colisão com a demanda por mão de obra qualificada em países desenvolvidos que estarão com suas populações diminuindo.\nO objetivo do canal e do blog é tentar auxiliar na melhoria de processos para que estejamos mais preparados quando chegarmos em 2040.\n\n\n\n\n\n\n\n Como serão divididos os tópicos ?\n\n\n\n\n\n\nClique aqui pra abrir os detalhes\n\n\n\n\n\n . Demanda, definição do problema, objetivos e métricas de sucesso - Contém a origem e tipos de necessidade que é o que torna todo o restante do trabalho necessário.\n . Dados - Contém conceitos com o objetivo de estimular a observação, abstração e entendimento sobre o que são dados.\n . Ferramentas - Contém ferramentas como R(RStudio), Oracle SQL*plus, Oracle SNO, SPSS, Sphinx, Qlikview, PowerBI e excel.\n . Exploração - Contém visualização de dados, análise exploratória, tabelas, queries, planilhas de excel, folhas de verificação etc.\n . Modelos - Contém alguns tipos de modelos descritivos, preditivos, inferenciais, otimização entre outros.\n . Versionamento e Reprodutibilidade - Contém processos para desenvolvimento de documentos reproduzíveis usando versionamento github, git, criação de documentos quarto_pub e rmardown, criação de livros bookdown entre outros.\n . Comunicação - Contém abordagens sobre comunicação das estatísticas e relatórios técnicos.\n . Aplicação - Contém histórico de ações e aplicações dos projetos.\n\n\n\n\n\n\n\n\n\n\n Como será divido cada post ?\nEm 2001 para facilitar o entendimento dos profissionais da operação em industria eu escolhi um padrão que ao longo do tempo vem se mostrando eficiente e utilizarei aqui.\n - O que é : qual a ideia do que está sendo abordado ?\n - Qual o objetivo : por que isso está sendo ou foi desenvolvido ?\n - De onde vem : quais as necessidades ou origem dessa demanda ? A motivação ?\n - Como fazer : qual o procedimento, código, sequência ou fluxo ?\n - Pra onde vai : quem é o consumidor disso ? Qual a aplicação prática ?\n - Qual o resultado : descrição dos resultados previstos ou comprovados."
  },
  {
    "objectID": "posts/comm-stats/comm_stat.html",
    "href": "posts/comm-stats/comm_stat.html",
    "title": "Como comunicar dados em artigos técnicos no R - Quarto",
    "section": "",
    "text": "O que é comunicação de dados\n\nEm um contexto mais geral, seria o processo relativo a transação de dados e ou informações entre 2 ou mais agentes.\nEm um contexto mais específico que será tratado nesse post é a demonstração de resultado técnico ou experimento em documentos(e.g. dossiê, poster scientific, blog, livro etc)\nMuito mais do que um documento, são ideias.\n\n\n\n Qual o objetivo\n\nDar suporte para ação adequada/eficiente em situações específicas usando sinais que podem ser visuais como escrita, fórmulas, gráficos e código reproduzível.\nFacilitar a aplicação do conceito techinical writing , facilitar a compreenção e por consequencia a busca por melhorias.\n\n\n\n De onde vem a demanda\n\nNecessidade de compreenção, reprodução ou melhoria de um processo.\nEscasses de recursos vs necessidade de melhoria da produtividade global dos fatores.\nNecessidade de transparência, credibilidade e consistência.\n\n\n\n\n\n\n\n\n\nModelo de rodovidas interestaduais nos Estados Unidos iniciado em 1919 e finalizado 1995.\n\n\n\n\n\n\n\nModelo de suburbio 1947-1951 Levittown Long Island Estados Unidos, modelo que está sendo usado até 2022.\n\n\n\n\n\n\nFigura 1: Exemplo de ações implementadas através do uso de reprodutibilidade de experimentos e ideias.\n\n\n\n\n\n Como fazer\n\nFerramenta: Quarto, rmarkdown, jupyter notebook, word, excel.\n\n\n\n\n\n\n\n\n\n\n\nFigura 2: Processo de comunicação de dados técnicos\n\n\n\n\nConceito: Uso de elementos que facilitem processo de comunicação como equations, citations, cross-reference, footnotes, embedded code, code chunck, inline code.\nPara reproduzir os códigos abaixo serão necessários os pacotes tidyverse e quarto\nPara instalar o quarto pub clique no link acima e siga os procedimentos(next&gt;next&gt;next). Após a instalação a opção criar um novo documento quarto vai aparecer automaticamente no RStudio.\n\n\n\n\n\n\n\n\n\nPara criar um novo documento usando o quarto\n\n\n\n\n\n\n\n\n\n\n(a) Para escolher o formato que o documento será gerado. No caso de escolher PDF será necessário instalar o pacote tinytex (demora um pouco)\n\n\n\n\n\n\n\nFigura 3: Gerando novo documento usando quarto_pub\n\n\n\n\nCódigo descrito no vídeo aqui\n\n\n\n Pra onde vai quem é o cliente\n\nPublicação intranet\nGithub\nZenodo\nKaggle\nPaperswithcode\nNCBI - National Center for Biotecnology Information\nServiço Brasileiro de Respostas técnicas\nBlog/Books/Poster\n\n\n\n Qual o resultado\n\nMelhoria da articulação permitindo expor com mais clareza os detalhes técnicos e os benefícios a respeito da ideia proposta.\nReutilização da ideia original (ao invés de partir do zero) mas incrementando melhorias, reduzindo tempo de implementação e se livrando de obstaculos ao longo do caminho consequentemente poupando recursos o que remete a continuidade e consistência.\nIncremento de qualidade e acessibilidade ao conheicmento, bens ou serviços resultando da melhoria das padrão de vida pessoas."
  },
  {
    "objectID": "posts/decision-tree-rpart/dtrpart.html",
    "href": "posts/decision-tree-rpart/dtrpart.html",
    "title": "Modelo preditivo | tidymodels decision tree rpart diabetes",
    "section": "",
    "text": "Vídeo tema para este post em Tidymodels decision tree model diabetes - rpart\n\n\n\n\n O que é isto?\n\nDecision tree models são modelos árvore de decisão baseadas em instruções if-then/e se utilizando os dados preditores. @kuhn2013, chap 8\nExemplo: if temperatura_corporal &gt;= 38 and dor_no_corpo = SIM then doente = SIM else doente = NAO\nO modelo que iremos usar é o rpart também conhecido por particionamento recursivo.\n\n\n\n Qual é o objetivo?\n\nPrever a probabilidade de um paciente ter diabetes com base em medidas de diagnóstico incluídas no conjunto de dados.\nTodos os pacientes são mulheres de no mínimo 21 anos de idade e descendencia de Indios Pima\nNos dados temos variáveis preditoras (independentes) e uma variável de resposta (dependente).\nVariáveis independentes incluem o número de gestações, massa(IMC), nível de insulina, idade, espessura da pele entre outras.\n\n\n\n De onde vem a demanda?\n\nNecessidade de praticar a construção de modelos machine learning usando o framework tidymodels.\n\n\n\n Como fazer isto?\nPacotes\nPara reproduzir os códigos abaixo serão necessários os pacotes tidyverse , tidymodels e janitor\nDados\nO conjunto de dados é o Pima Indians Diabetes Database disponível na Universidade da California Irvine Machine Learning Repository, no kaggle e também no pacote mlbench no R.\n\n\n\n\n\n\nClique aqui para mostrar dados complementares sobre Pima Indian Diabetes\n\n\n\n\n\n\nPima US são povos indígenas, e hoje vivem principalmente em três reservas no Arizona: A Reserva do Rio Gila, A Reserva do Rio Salgado, A Reserva Ak-Chin\nOs índios Pima do Arizona são estudados por mais de 30 anos e intrigam os pesquisadores porque sofrem de uma das taxas mais altas de diabetes do mundo.\nEntre os índios Pima mexicanos, 5,6% dos homens e 8,5% das mulheres tinham diabetes, enquanto nos índios Pima do US 34,2% dos homens e 40,8% das mulheres tinham a doença (P &lt; 0,01) prevalências significativamente maior nos americanos.\nCerca de metade dos Pimas do Arizona com 40 anos ou mais tem diabetes de início adulto, uma condição na qual a insulina é produzida em quantidades insuficientes para atender às necessidades do corpo.\nEstão severamente acima do peso sendo os jovens estão acima da média nos EUA onde 1 a cada 4 são considerados obesos.\nOutro problema no grupo e insuficiência renal decorrente do diabetes sendo que 60% dos Pimas do Arizona desenvolvem doença renal relacionada ao diabetes contra 30% da população americana.\nExpectativa de vida dos Pima é muito menor do que a média nacional 72/M 78/F. Nos Pima 53/M e 63/F\n\n\n\n\nFluxo de trabalho\n\nCódigo reproduzível\n\n\n\nPackages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n\n\n\nData\n\ndata(PimaIndiansDiabetes, package = \"mlbench\")\n\ndata_pima &lt;- PimaIndiansDiabetes %&gt;% janitor::clean_names()\n\ndata_pima %&gt;% funModeling::df_status()\ndata_pima %&gt;% funModeling::plot_num()\n\ndata_pima_clean &lt;- \n  data_pima %&gt;% \n  mutate(across(c(\"pregnant\":\"mass\"), ~ifelse(.x==0,NA,.x))) \n\n\n\n\n\nExplore\n\ndata_pima_clean %&gt;% \n  select(glucose, insulin, mass, diabetes) %&gt;% \n  GGally::ggpairs(aes(color = diabetes, alpha =0.3))\n\n\n\n\n\nSplit the data\n\nsplit_pima &lt;- initial_split(data_pima_clean, strata = diabetes)\ntrain_pima &lt;- training(split_pima)\ntest_pima &lt;- testing(split_pima)\n\nresample_pima &lt;- vfold_cv(train_pima, strata = diabetes, v=5)\n\n\n\n\n\nPre processing\n\nrec_pima &lt;- \n  recipe(diabetes~., data = train_pima) %&gt;% \n  step_impute_knn(all_predictors()) \n\n\n\n\n\nModel Specification\n\nmdl_spec_rpart_pima &lt;- \n  decision_tree() %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_args(cost_complexity= tune(),\n           tree_depth = tune(),\n           min_n = tune())\n\n\n\n\n\nModel Workflow\n\nwkfl_rpart_pima &lt;- \n  workflow() %&gt;% \n  add_recipe(rec_pima) %&gt;% \n  add_model(mdl_spec_rpart_pima)\n\n\n\n\n\nHyperparameter tune\n\n# grid spec      ---------------------------------------------------------------\ngrid_pima &lt;- \n  grid_regular(cost_complexity(),\n               tree_depth(),\n               min_n(),\n               levels = 3)\n\n# tune grid      ---------------------------------------------------------------\ndoParallel::registerDoParallel()\n\nset.seed(123)\ntune_grid_rpart_pima &lt;- \n  tune_grid(wkfl_rpart_pima,\n            resamples = resample_pima,\n            grid = grid_pima,\n            metrics = metric_set(roc_auc,accuracy),\n            control = control_grid(save_pred = TRUE))\n\nbest_grid_rpart_pima &lt;- tune_grid_rpart_pima %&gt;% select_best(metric= \"roc_auc\")\n\n#final wkfl\nfinal_wkfl_rpart_pima &lt;- \n  finalize_workflow(wkfl_rpart_pima, best_grid_rpart_pima)\n\n\n\n\n\nModel Evaluating\n\nmdl_eval_rpart_pima &lt;- \n  final_wkfl_rpart_pima %&gt;% \n  last_fit(split_pima)\n\nmdl_eval_rpart_pima %&gt;% \n  extract_fit_engine() %&gt;% \n  rpart.plot::rpart.plot(cex = 0.6,\n                         type = 3,\n                         roundint = FALSE)\n\n\n\n\n\nModel Fit\n\nmdl_fit_rpart_pima &lt;- \n  final_wkfl_rpart_pima %&gt;% \n  fit(data_pima_clean)\n\n\n\n\n\nMake predictions\n\nnew_data_pima &lt;- \n  tribble(~pregnant, ~glucose, ~pressure,~triceps,~insulin, ~mass, ~pedigree, ~age,\n          2,87,68,34,77,38,0.41,25)\n\npredict(mdl_fit_rpart_pima, new_data = new_data_pima)\n\n\n\n\n\nVariable Importance\n\nmdl_fit_rpart_pima %&gt;% \n  extract_fit_engine() %&gt;% \n  vip::vip()\n\n\n\n\n Quem é o consumidor?\n\nRepositório de documento criados para praticar o desenvolvimento de modelos de previsão usando tidymodels.\n\n\n\n Qual o resultado?\n\nAperfeiçoamento das técnicas de construção de modelos usando tidymodels.\nCriação de modelo de previsão que pode ser melhorado e aproveitado projetos futuros.\nEstimular a geração de ideias, dúvidas e conhecimento acerca do problema e também na construção de modelos de previsão."
  },
  {
    "objectID": "posts/presentation_reveljs/presentation.html",
    "href": "posts/presentation_reveljs/presentation.html",
    "title": "Comunicar trabalhos técnicos em apresentações no R com Quarto",
    "section": "",
    "text": "Vídeo tema para este post em Comunicar trabalhos técnicos em apresentações no R com Quarto\n\n\n\n\n O que é isto?\n\nComo fazer relatório técnico e científico usando Quarto e reveljs para publicações no formado de apresentações de slides.\n\n\n\n Qual é o objetivo?\n\nFacilitar a comunicação de experimentos feitos por pesquisadores ou melhoristas de processos.\n\n\n\n De onde vem a demanda?\n\nDesenvolver pesquisas que possas ser reproduzidas por outros pesquisadores.\nDirecionar mais esforços para o desenvolvimento do conteúdo da pesquisa e menos para ferramentas de apresentação.\n\n\n\n Como fazer isto?\nPacotes\nPara reproduzir o código apresentado neste exercício, os seguintes pacotes são necessário: tidyverse, sjplot, gt, gtExtras, e funModeling\nDados\nO conjunto de dados que será usado aqui é o gapminder disponível no pacote gapminder no R e pode ser acessado via código install.package(\"gapminder)\nTime line das ferramentas disponíveis ao longo do tempo\n\nCódigo reproduzível\nO YML file é o cabeçalho do documento onde são inseridos o título, subtítulo entre outros detalhes do documento como por exemplo as configurações globais que serão reproduzidas nas demais partes do documento.\n\n\n\nyml\n\n---\ntitle: \"Minha pesquisa\"\nauthor: \"Marcelo Carvalho\"\nformat: \n  revealjs:\n    chalkboard: true\n    multiplex: true\neditor: visual\nexecute:\n  echo: true\n---\n\n\nO resumo geralmente consiste em textos e pequenos códigos inline\n\n\n\nresumo\n\n## Resumo - SLIDE1\n\n-   Apresentações são formas comums de comunicação de resultados de expementos\n\n-   Existem muitas ferramentas disponíveis\n\n-   Este exercício vai mostrar algumas `quarto` e `reveljs`\n\n## Objetivo - SLIDE2\n\n-   Facilitar a publicação de experimentos realizados por pesquisadores e melhoristas de processos\n\n## Pacotes - SLIDE3\n\n-   Neste trabalho foram usados os seguintes pacotes do R\n\n#Código inserido no code-chunk\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(janitor)\nlibrary(gapminder)\n\n#Resultado gerado\n#&gt; 1.  Facilita o manuseio de dados\n#&gt; 2.  Facilita a construção de modelos\n#&gt; 3.  Facilita a padronização dos nomes das variáveis\n#&gt; 4.  Concentra os dados que serão usados no exercício\n\n\nO corpo consiste em código reproduzível\n\n\n\ncorpo\n\n## Dados - SLIDE4\n-   A base de dados utilizada será a `Gapminder`\n-   Possui `r nrow(gapminder)` linhas e `r ncol(gapminder)` colunas.\n-   Os detalhes da estrutura são apresentados abaixo\n\n## gerar os detalhes da estrutura de dados\ngapminder %&gt;% glimpse()\n\n\n## Exploração - SLIDE5\ngapminder %&gt;% funModeling::df_status()\n\n\n## Método - SLIDE6\n\n$$\nY_i= \\beta_0 + \\beta_1 X_i + \\epsilon_i\n$$\n\n$Y_i$= Variável dependente\n\n$\\beta_0$= Constante ou Intercept\n\n$\\beta_1$= Coeficiente ou Slope\n\n$X_i$= Variável independente\n\n$\\epsilon_i$= Erro (desvio)\n\n\n## Modelo - SLIDE7\nmdl_gapminder &lt;- \n  gapminder %&gt;% \n  mutate(qty_year = year - 2007) %&gt;% \n  group_nest(country) %&gt;% \n  mutate(mdl = map(data, ~lm(lifeExp~qty_year, data = .x))) %&gt;% \n  mutate(result = map(mdl, broom::glance)) %&gt;% \n  select(country, result) %&gt;% \n  unnest(result)\n\n\n\n## Plotar modelo - SLIDE8\n#| output-location: slide\n#| code-line-numbers: \"2|4\"\nmdl_gapminder %&gt;% \n  ggplot(aes(x = r.squared, y = fct_reorder(country, r.squared)))+\n  geom_point()+\n  scale_y_discrete(guide = guide_axis(check.overlap = TRUE))+\n  labs(x = NULL, y = NULL)\n\n## Conclusão - SLIDE9\n-   `r mdl_gapminder %&gt;% filter(country == \"Brazil\") %&gt;% select(r.squared) %&gt;% \nmutate(r.squared = scales::percent(r.squared, accuracy = 0.1))` da \nvariação na expectativa de vida no Brasil é explicada pela variação do tempo.\n-   Consigo aprofundar nas pesquisas para compreender se há outros fatores que talvez possam explicar as correlações ?\n\n## Complemento1 - SLIDE10\n#| output-location: column\nlibrary(gt)\nlibrary(gtExtras)\n\ndata(FANG, \n     package = \"tidyquant\")\n\ndata_fang &lt;- FANG\n\ndata_fang %&gt;% group_by(symbol) %&gt;%  \n  summarise(price = list(adjusted)) %&gt;% \n  gt() %&gt;% \n  gt_plt_sparkline(\n    price,\n    same_limit = FALSE,\n    fig_dim = c(20,40),\n    type = \"ref_median\")\n\n## Complemento2 - SLIDE11\nlibrary(sjPlot)\n\nmdl_mtcars &lt;- lm(mpg~wt + cyl, data = mtcars)\nsjPlot::tab_model(mdl_mtcars)\n\n\n\n\n Quem é o consumidor?\n\nPesquisadores e profissionais que precisam comunicar suas pesquisas e experimentos.\nMelhoristas de processo que precisam de documentos reproduzíveis em seus projetos.\n\n\n\n Qual o resultado?\n\nAperfeiçoar habilidades na comunicação dos resultados dos experimentos..\nFacilitar a comunicação entre pesquisadores. Tidyverse usa design for humans %\\&gt;% significa e então or and then.\nMelhorar produtividade na manufatura e serviços através da replicação dessas ações e experimentos."
  },
  {
    "objectID": "posts/resamples/resamples.html",
    "href": "posts/resamples/resamples.html",
    "title": "Modelo preditivo | Amostra e reamostragens",
    "section": "",
    "text": "Vídeo tema para este post em Amostra e reamostragem com Rsample\n\n\n\n\n O que é isto?\n\nÉ a separação de um conjunto de dados em porções as quais serão usadas para diferentes etapas do modelo.\nA separação de um conjunto de dados em amostra de treino e teste é padrão na construção de modelos de machine learning e deep learning\n\n\n\n Qual é o objetivo?\n\nMelhorar a acuracidade dos modelos é um dos principais objetivos uma vez que alguns modelos se adaptam muito rapidamente aos padrões dos dados de treino(decoram) e se estes forem usados para teste podem ter a acuracidade comprometida.\nAs técnicas de reamostragem auxiliam no equilibro entre bias e variance\n\n\n\n De onde vem a demanda?\n\nMelhorar o desempenho e acuracidade dos modelos.\n\n\n\n Como fazer isto?\nPacotes\nPara reproduzir os códigos abaixo serão necessários os pacotes tidyverse , tidymodels,tidyquant e quantmod\nDados\nForam usados dados do conjunto ames housing,DGS10eEWZcomo exemplo. Todos eles são possíveis acessar através dos códigos descritos abaixo.\nEtapa do projeto\n\nCódigo reproduzível\n\n\n\nPackages\n\n# packages ----------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tidyquant)\nlibrary(quantmod) #freadR\nlibrary(scales)\n\n\n\n\n\nModels\n\n# data --------------------------------------------------------------------\n#ames housing case\ndata(ames)\n\ndata_ames &lt;- \n  ames %&gt;%\n  mutate(Sale_Price = log10(Sale_Price))\n\n# simple split ------------------------------------------------------------\nsplit_ames &lt;- initial_split(ames, strata = Sale_Price)\ntrain_ames &lt;- training(split_ames)\ntest_ames &lt;- testing(split_ames)\n\n# validation split --------------------------------------------------------\nsplit_val_ames &lt;- initial_validation_split(ames, strata = Sale_Price)\ntrain_val_ames &lt;- training(split_val_ames)\nval_ames &lt;- validation(split_val_ames)\ntest_val_ames &lt;- testing(split_val_ames)\n\n# time split --------------------------------------------------------------\ndata_10y &lt;- tq_get(\"DGS10\", get = \"economic.data\")\n\ndata_10y %&gt;% \n  ggplot(aes(x = date, y = price))+\n  geom_line()\n\nsplit_10y &lt;- initial_time_split(data_10y)\ntrain_10y &lt;- training(split_10y)\ntest_10y &lt;- testing(split_10y)\n\ntrain_10y %&gt;% tail()\ntest_10y %&gt;% head()\n\nc(max(train_10y$date), min(test_10y$date))\n\n# time split lag ----------------------------------------------------------\nsplit_10y_lag &lt;- initial_time_split(data_10y, lag = 20)\ntrain_10y_lag &lt;- training(split_10y_lag)\ntest_10y_lag &lt;- testing(split_10y_lag)\n\ntrain_10y_lag %&gt;% tail()\ntest_10y_lag %&gt;% head()\nc(max(train_10y_lag$date), min(test_10y_lag$date))\n\n# resample simple ---------------------------------------------------------\nresample_ames &lt;- vfold_cv(train_ames, strata = Sale_Price, v=10)\nresample_ames\n\n# resample time -----------------------------------------------------------\nresample_10y &lt;- rolling_origin(train_10y_lag,\n                               initial = 50,\n                               assess = 10)\n\n# resample group ----------------------------------------------------------\nset.seed(1353)\ncar_split &lt;- group_initial_split(mtcars, cyl)\ntrain_data &lt;- training(car_split)\ntest_data &lt;- testing(car_split)\n\n# resample time group -----------------------------------------------------\ngetSymbols(\"EWZ\")\n\nchartSeries(EWZ)\n\n#xts o df\nibov_usd &lt;- fortify.zoo(EWZ) %&gt;% janitor::clean_names()\n\n#reamostragem por grupo\nresample_ibov_usd &lt;- \n  ibov_usd %&gt;% \n  mutate(ym = as.POSIXlt(index)$year + 1900) %&gt;% \n  nest(data = c(-ym)) %&gt;% \n  rolling_origin(cumulative = FALSE)\n\nanalysis(resample_ibov_usd$splits[[2]]) \n\nassessment(resample_ibov_usd$splits[[2]]) \n\n\n\n\n Quem é o consumidor?\n\nAs amostras e reamostragens serão usadas durante a etapa de construção, ajuste e validação de modelos\n\n\n\n Qual o resultado?\n\nAperfeiçoamento das técnicas separação entre dados de treino, teste, validação e reamostragem mais adequados para cada caso.\nFacilitar o trabalho e melhorar o processo para o grupo responsável pelas próximas etapas no projeto.\nPermitir a aplicação prática dos resultados dos modelos através da melhoria de acuracidade e poder preditivo gerando benefícios para os usuários do modelo e para a sociedade."
  },
  {
    "objectID": "posts/cause-effect-diagram/causeeffect.html",
    "href": "posts/cause-effect-diagram/causeeffect.html",
    "title": "Diagrama de Causa e Efeito",
    "section": "",
    "text": "O que é o diagrama de causa e efeito\n\nO diagrama de causa e efeito é uma ferramenta que ajuda a representar com certa facilidade a estrutura do pensamento humano diante de situações e problemas complicados de se resolver.\nPor se tratar de uma ferramenta gráfica, facilita identificar uma relação significativa sob diversos pontos de vista entre o efeito e suas possíveis causas Werkema (2021) .\nConforme Kume (1987) construir um gráfico de causa e efeito que seja útil exige esforço, treinamento da equipe e tempo. Pessoas que tem sucesso na solução de problemas de controle de qualidade são aqueles bem sucedidas na construção de diagramas de causa e efeito úteis.\n\n\n\n\nFigura - Diagrama de causa e efeito\n\n\n\n\n Qual o objetivo\n\nEstimular a interação entre as pessoas, processo, instrumentos, ambiente e sistemas na busca de possíveis causas que podem unfluenciar ou gerar um determinado efeito.\nPermitir que todos os envolvidos contribuam com ideias e pontos de vista.\nFacilitar a identificação de possíveis causas e os pesos dessas causas aparentes sobre o efeito observado.\n\n\n\n De onde vem a demanda\n\nUm professor na universidade de Tokyo Kaoru Ishikawa, usou um diagrama para sintetizar opiniões de engenheiros nas fábricas durante discussões sobre qualidade em 1953. Conforme Kume (1987), alunos de Ishikawa também empregaram o método para organizar fatores em suas atividades de pesquisa.\nPela simplicidade e eficiência da ferramenta, ela foi adotada e incorporada aos JIS (Japanese Industrial Standards).\nDe acordo com Shatz (2024) , um viés cognitivo conhecido como a maldição do conhecimento faz com que as pessoas não consigam compreender adequadamente a perspectiva de quem não possui tantas informações quanto elas. Uma pessoa experiente em uma tarefa pode ter dificuldades para ensinar iniciantes por pressupor intuitivamente que as coisas são obvias para eles, logo também são óbvias para o iniciante, mesmo que não seja o caso. O diagrama de causa e efeito pode ser útil nesses casos, pois quando bem empregado, rompe a barreira do medo e coleta a perspectiva de vários membros da equipe sobre determinado problema resultados os quais, podem ser reveladores e muito úteis.\nObter pontos de vistas diferentes pode ser fundamental durante um processo de melhoria contínua. Conforme Helena (2022) , tente imaginar um fato Um veículo bateu em um poste e imagine também algumas versões desse fato.\n\nVersão física: Um corpo foi machucado, um carro amassado e um poste derrubado.\nVersão economica: O custo do medicamento, o custo do carro, o custo do poste, o tempo perdido, trabalho necessário para recompor o ambiente.\nVersão social: Pessoas abaladas, assustadas, uma criança que passava no local chorou muito, e o motorista ficou em estado de choque.\nVersão analítica: A velocidade não era adequada para pista, a posição do sinaleiro não dava uma boa visibilidade para o motorista, o automóvel tem uma característica mecanica que provoca frenagem muito brusca.\nVersão ampla: Considerar as versões anteriores e trabalhar em uma decisão que beneficie os envolvidos, vendo a parcela de responsabilidade de cada um e como o problema pode ser resolvido com o mínimo dano. A ideia é reduzir os impactos negativos para a sociedade, ao memo tempo impedir que motorista seja massacrado e penalizado por aquilo que não tenha sido de seu controle. Que situações desse tipo, possam ser corrigidas e melhoradas minimizando ou neutralizando os fatores responsáveis pelo problema e reduzindo a probabilidade que o mesmo ocorra no futuro.\n\n\n\n\n Como fazer\n\nEm uma ponta você colocar o efeito, e as ideias sobre as possíveis causas aparentes são coletadas de times multidisciplinares.\nVocê pode usar uma folha de papel e se preferir usar o R você pode usar o pacote qcc criado por (scrucca2004?)\n\n\nlibrary(qcc)\n\ncause.and.effect(\n  cause = list(Equipamento = c(\"Problema freq. \\n na maquina \\n de montar caixas\"),\n               Processo = c(\"Manual \\n desatualizado\"),\n               Pessoas = c(\"Rotatividade \\n de pessoal\"),\n               Materiais = c(\"Embalagens com \\n padrão diferente\"),\n               Ambiente = c(\"Variação brusca \\n de temperatura\"),\n               Gestão = c(\"Indicador inadequado\")),\n  effect=\"Atrazo na \\n entrega\",\n  title = \"Diagrama de causa e efeito\")\n\n\n\n\n\n\n\n\n\n\n Pra onde vai quem é o cliente\n\nOs resultados coletados na análise do diagrama de causa e efeito são conhecidas como causas aparentes e são utilizados na identificação da causa raiz onde é possível usar a técnica de cinco porques. Enquanto a técnica dos cinco porques tem o foco na profundidade das relações de causa e efeito, o diagrama de causa e efeito tem o foco nas diferentes perspectivas nessas relações. Em resumo, mesmo os pontos de vista sendo diferentes por serem coletados por diferentes membros da equipe, eles tendem a convergir em relação ao problema. Por exemplo se considerarmos que a causa aparente do atrazo na entrega seja problemas frequentes na maquina de montar caixa. Quando usamos a técnica de cinco porques, poderia ficar assim:\n\nPor que a maquina de montar caixas tem problemas frequentes?\n\nPor que as caixas possuem formatos diferentes, estão abauladas e as ventosas de sucção da maquina se desgastam rapidamente\n\nPor que as caixas vem com padrão diferente?\n\nPor que elas vem de plantas e fornecedores diferentes e sem controle muito rígido de forma e uniformidade.\n\nPor que não tem controle de uniformidade?\n\nPor que há necessidade de redução de custos e as compras são feitas em lotes muito grandes e com base no preço.\n\nPor que as compras são feitas com base no preço?\n\nPor que a consultoria que nos apóia e que sugeriu o procedimento tem sua receita baseada na economia que ela faz. Quanto mais custos ela cortar, melhor ela será recompensada mesmo que sem a intenção isso possa ampliar os problemas no longo prazo para o nosso negócio, portanto, o indicador é inadequado e é um problema de gestão.\n\n\n\n\n\n Qual o resultado\n\nPossibilita coletar, analisar e compreender os efeitos e possíveis causas de um problema sob diversos pontos de vista.\nPossibilia maior cooperação e participação de todos na busca da melhoria contínua dos processos.\nGeram dados importantes para estratégias de ação e comunicação a cerca de um determinado problema.\n\n\n\n\n\n\nReferences\n\nHelena, Lucia. 2022. “Educação Midiática: Uma visão filosófica - Nova Acrópole.” https://www.youtube.com/.\n\n\nKume, Hitoshi. 1987. Statistical Methods for Quality Improvement. 1ª edição. Tokyo: Productivity Press.\n\n\nShatz, Itamar. 2024. “The Curse of Knowledge: A Difficulty in Understanding Less-Informed Perspectives  Effectiviology.” https://effectiviology.com/curse-of-knowledge/.\n\n\nWerkema, Cristina. 2021. Métodos PDCA e Demaic e Suas Ferramentas Analíticas. 1ª edição. GEN Atlas."
  },
  {
    "objectID": "posts/table-gt/table_gt.html",
    "href": "posts/table-gt/table_gt.html",
    "title": "Formatar tabelas para publicar artigos no R com GT package",
    "section": "",
    "text": "O que é o pacote GT\n\nÉ um pacote com funções úteis para criação de tabelas em documentos Iannone et al. (2024).\nTem como base o princípio tidy que busca o equilibrio na interface homem maquina, homem homem e reprodutibilidade.\n\n\n\n Qual o objetivo\n\nFacilitar a construção de tabelas em documentos técnicos e científicos.\nPermitir a construção de ampla variedade de tabelas.\n\n\n\n De onde vem a demanda\n\nDa elaboração de documentos reproduzíveis.\nDa simplicidade e código mais legível na produção de documentos técnicos e científicos.\n\n\n\n Como fazer\n\nPara o exemplo será usado o conjunto dados gapminder . Se não tiver o pacote gapminder instalado, para instalar é install.packages(\"gapminder\").\n\n\n# packages ----------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gapminder)\n\n\n# tbl ---------------------------------------------------------------------\n\ngapminder %&gt;% \n  filter(year == 2007 & lifeExp &gt;81) %&gt;% \n  gt(groupname_col = \"continent\") %&gt;% \n  tab_header(title = md(\"**Tabela1: Expectativa de vida vs renda**\"),\n             subtitle = \"Apenas países com exp &gt; 81\") %&gt;% \n  tab_options(heading.align = \"left\",\n              column_labels.border.top.color = \"black\",\n              column_labels.border.top.width = px(3)) %&gt;% \n  cols_align(align = \"right\", columns = 1:4) %&gt;% \n  fmt_number(columns = c(lifeExp, pop),\n             decimals = 0) %&gt;% \n  fmt_currency(columns = c(gdpPercap),\n               currency = \"USD\") %&gt;% \n  tab_footnote(\n    footnote = \"Renda menor que a média restante\",\n    locations = cells_body(columns = c(gdpPercap),\n                           rows = gdpPercap &lt; 35000)) %&gt;% \n  tab_footnote(\n    footnote = \"GDP(PPP) purchasing power parity\",\n    locations = cells_column_labels(columns = c(gdpPercap))\n  ) %&gt;% \n  tab_source_note(md(\"**Fonte**: Gapminder\"))\n\n\n\n\n  \n    \n      Tabela1: Expectativa de vida vs renda\n    \n    \n      Apenas países com exp &gt; 81\n    \n    \n      country\n      year\n      lifeExp\n      pop\n      gdpPercap1\n    \n  \n  \n    \n      Oceania\n    \n    Australia\n2007\n81\n20,434,176\n2 $34,435.37\n    \n      Asia\n    \n    Hong Kong, China\n2007\n82\n6,980,412\n$39,724.98\n    Japan\n2007\n83\n127,467,972\n2 $31,656.07\n    \n      Europe\n    \n    Iceland\n2007\n82\n301,931\n$36,180.79\n    Switzerland\n2007\n82\n7,554,661\n$37,506.42\n  \n  \n    \n      Fonte: Gapminder\n    \n  \n  \n    \n      1 GDP(PPP) purchasing power parity\n    \n    \n      2 Renda menor que a média restante\n    \n  \n\n\n\n\n\n\n Pra onde vai quem é o cliente\n\nComunicação de pesquisas técnicas e científicas.\nManuais de treinamento de equipe.\nPortal de comunicação de informações do negócio.\n\n\n\n Qual o resultado\n\nFacilita a construção de tabela e melhor comunicação dos dados.\nPossibilia a reprodução rápida e prática da tabela através da reutilização e adaptação dos códigos usadas na construção por outros membros do time.\nMelhora a consistência na apresentação de dados reduzindo os riscos de erros de digitação, entre outras inconsistências.\n\nVídeo tema para este post\n\n\n\n\n\n\n\nReferences\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, and JooYoung Seo. 2024. “Gt: Easily Create Presentation-Ready Display Tables.” https://CRAN.R-project.org/package=gt."
  },
  {
    "objectID": "posts/gts-themes/gts-themes.html",
    "href": "posts/gts-themes/gts-themes.html",
    "title": "Formatando tabelas e dados usando themes e gtsummary package no R",
    "section": "",
    "text": "O que são themes no pacote gtsummary\n\nSão funções que criam presets de tabelas para criação de documentos técnicos.\nA função faz parte do pacote gtsummary. Mas informações sobre o pacote Sjoberg et al. (2021).\n\n\n\n Qual o objetivo\n\nOs temas são úteis na criação de tabelas impactando nas características visuais e funcionais em uma tabela.\nDar praticidade para criação de tabelas em documentos técnicos\n\n\n\n De onde vem a demanda\n\nNecessidade de adequação das apresentações conforme requisitos de revistas e ou instituições responsáveis pela publicação de conteúdo técnico.\nFacilitar a construção rápida de tabelas conforme padrões pré determinados.\n\n\n\n Como fazer\n\nTheme compact\n\n#get packages\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(gtExtras)\nlibrary(survival)\nlibrary(survminer)\nlibrary(labelled)\n\n\n#get data\ndata_iris &lt;- iris %&gt;% janitor::clean_names()\ndata_mtcars &lt;- mtcars\n\n#compact theme\nreset_gtsummary_theme()\ntheme_gtsummary_compact()\n\ndata_iris %&gt;% tbl_summary(by = species)\n\n\n\n\n\n\n\nCharacteristic\nsetosa, N = 501\nversicolor, N = 501\nvirginica, N = 501\n\n\n\n\nsepal_length\n5.00 (4.80, 5.20)\n5.90 (5.60, 6.30)\n6.50 (6.23, 6.90)\n\n\nsepal_width\n3.40 (3.20, 3.68)\n2.80 (2.53, 3.00)\n3.00 (2.80, 3.18)\n\n\npetal_length\n1.50 (1.40, 1.58)\n4.35 (4.00, 4.60)\n5.55 (5.10, 5.88)\n\n\npetal_width\n0.20 (0.20, 0.30)\n1.30 (1.20, 1.50)\n2.00 (1.80, 2.30)\n\n\n\n1 Median (IQR)\n\n\n\n\n\n\n\n\n\n\nTheme EDA exploratory data analysis\n\n#eda theme\nreset_gtsummary_theme()\ntheme_gtsummary_eda()\n\ndata_iris %&gt;% tbl_summary(by = species)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nsetosa, N = 50\nversicolor, N = 50\nvirginica, N = 50\n\n\n\n\nsepal_length\n\n\n\n\n\n\n\n\n    Median (IQR)\n5.00 (4.80, 5.20)\n5.90 (5.60, 6.30)\n6.50 (6.23, 6.90)\n\n\n    Mean (SD)\n5.01 (0.35)\n5.94 (0.52)\n6.59 (0.64)\n\n\n    Range\n4.30, 5.80\n4.90, 7.00\n4.90, 7.90\n\n\nsepal_width\n\n\n\n\n\n\n\n\n    Median (IQR)\n3.40 (3.20, 3.68)\n2.80 (2.53, 3.00)\n3.00 (2.80, 3.18)\n\n\n    Mean (SD)\n3.43 (0.38)\n2.77 (0.31)\n2.97 (0.32)\n\n\n    Range\n2.30, 4.40\n2.00, 3.40\n2.20, 3.80\n\n\npetal_length\n\n\n\n\n\n\n\n\n    Median (IQR)\n1.50 (1.40, 1.58)\n4.35 (4.00, 4.60)\n5.55 (5.10, 5.88)\n\n\n    Mean (SD)\n1.46 (0.17)\n4.26 (0.47)\n5.55 (0.55)\n\n\n    Range\n1.00, 1.90\n3.00, 5.10\n4.50, 6.90\n\n\npetal_width\n\n\n\n\n\n\n\n\n    Median (IQR)\n0.20 (0.20, 0.30)\n1.30 (1.20, 1.50)\n2.00 (1.80, 2.30)\n\n\n    Mean (SD)\n0.25 (0.11)\n1.33 (0.20)\n2.03 (0.27)\n\n\n    Range\n0.10, 0.60\n1.00, 1.80\n1.40, 2.50\n\n\n\n\n\n\n\n\n\nTheme journal\n\nreset_gtsummary_theme()\ntheme_gtsummary_journal(journal = \"jama\")\ndata_iris %&gt;% tbl_summary(by = species)\n\nreset_gtsummary_theme()\ntheme_gtsummary_journal(journal = \"lancet\")\ndata_iris %&gt;% tbl_summary(by = species)\n\nreset_gtsummary_theme()\ntheme_gtsummary_journal(journal = \"nejm\")\ndata_iris %&gt;% tbl_summary(by = species)\n\n\nreset_gtsummary_theme()\ntheme_gtsummary_journal(journal = \"qjecon\")\ndata_iris %&gt;% tbl_summary(by = species)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nsetosa, N = 501\nversicolor, N = 501\nvirginica, N = 501\n\n\n\n\nsepal_length\n5.00 (4.80, 5.20)\n5.90 (5.60, 6.30)\n6.50 (6.23, 6.90)\n\n\nsepal_width\n3.40 (3.20, 3.68)\n2.80 (2.53, 3.00)\n3.00 (2.80, 3.18)\n\n\npetal_length\n1.50 (1.40, 1.58)\n4.35 (4.00, 4.60)\n5.55 (5.10, 5.88)\n\n\npetal_width\n0.20 (0.20, 0.30)\n1.30 (1.20, 1.50)\n2.00 (1.80, 2.30)\n\n\n\n1 Median (IQR)\n\n\n\n\n\n\n\n\n\n\nTheme language\n\nreset_gtsummary_theme()\ntheme_gtsummary_compact()\ntheme_gtsummary_language(language = \"pt\")\n\ndata_iris %&gt;% tbl_summary(by = species) %&gt;% add_p()\n\n\n\n\n\n\n\nCaracterísticas\nsetosa, N = 501\nversicolor, N = 501\nvirginica, N = 501\nValor p2\n\n\n\n\nsepal_length\n5.00 (4.80, 5.20)\n5.90 (5.60, 6.30)\n6.50 (6.23, 6.90)\n&lt;0.001\n\n\nsepal_width\n3.40 (3.20, 3.68)\n2.80 (2.53, 3.00)\n3.00 (2.80, 3.18)\n&lt;0.001\n\n\npetal_length\n1.50 (1.40, 1.58)\n4.35 (4.00, 4.60)\n5.55 (5.10, 5.88)\n&lt;0.001\n\n\npetal_width\n0.20 (0.20, 0.30)\n1.30 (1.20, 1.50)\n2.00 (1.80, 2.30)\n&lt;0.001\n\n\n\n1 Mediana (AIQ)\n\n\n2 Teste de Kruskal-Wallis\n\n\n\n\n\n\n\n\n\n\nTheme sd theme\n\nreset_gtsummary_theme()\ntheme_gtsummary_mean_sd()\n\ndata_iris %&gt;% tbl_summary(by = species)\n\n\n\n\n\n\n\nCharacteristic\nsetosa, N = 501\nversicolor, N = 501\nvirginica, N = 501\n\n\n\n\nsepal_length\n5.01 (0.35)\n5.94 (0.52)\n6.59 (0.64)\n\n\nsepal_width\n3.43 (0.38)\n2.77 (0.31)\n2.97 (0.32)\n\n\npetal_length\n1.46 (0.17)\n4.26 (0.47)\n5.55 (0.55)\n\n\npetal_width\n0.25 (0.11)\n1.33 (0.20)\n2.03 (0.27)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\n\nTheme custom\nNeste exemplo do livro de Kume (1993), queremos saber se há diferença entre o peso de pães que são fabricados em diferentes maquinas (maquina 1 e maquia 2)\n\ndata_pad &lt;- read.csv(\"https://raw.githubusercontent.com/factorialmap/hitoshi_kume_database/main/data_baker_66.csv\")\n\n\ndata_pad %&gt;% \n  pivot_longer(cols = -c(dia, padeiro),\n               names_to = c(\"maquina\",\"amostra\"),\n               names_sep = \"_\",\n               values_to = \"peso\") %&gt;% \n  tbl_summary(by = maquina,\n              include = peso) %&gt;% \n  add_difference() %&gt;% \n  as_gt() %&gt;% \n  tab_header(title = md(\"**Experimento maquina de pães 1 e 2**\"),\n             subtitle = \"Peso dos pães em gramas\") %&gt;% \n  tab_options(heading.align = \"left\",\n              column_labels.border.top.color = \"black\",\n              column_labels.border.top.width = px(3))\n\n\n\n\n\n\n\nExperimento maquina de pães 1 e 2\n\n\nPeso dos pães em gramas\n\n\nCharacteristic\nmaquina1, N = 801\nmaquina2, N = 801\nDifference2\n95% CI2,3\np-value2\n\n\n\n\npeso\n204.9 (4.6)\n210.5 (3.6)\n-5.6\n-6.9, -4.3\n&lt;0.001\n\n\n\n1 Mean (SD)\n\n\n2 Welch Two Sample t-test\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\n Pra onde vai quem é o cliente\n\nComunicação de pesquisas técnicas e científicas.\nManuais de treinamento de equipe.\nPortal de comunicação de informações do negócio.\n\n\n\n Qual o resultado\n\nFacilita a construção de tabela e melhor comunicação dos dados.\nPossibilia a reprodução rápida e prática da tabela.\nMelhora a consistência na apresentação de dados.\n\nVídeo tema para este post\n\n\n\n\n\n\n\nReferences\n\nKume, Hitoshi. 1993. Metodos Estatisticos Para Melhoria Da Qualidade. 9ª edição. Gente.\n\n\nSjoberg, Daniel D., Karissa Whiting, Michael Curry, Jessica A. Lavery, and Joseph Larmarange. 2021. “Reproducible Summary Tables with the Gtsummary Package.” The R Journal 13: 570–80. https://doi.org/10.32614/RJ-2021-053."
  },
  {
    "objectID": "posts/gts-print/gts-print.html",
    "href": "posts/gts-print/gts-print.html",
    "title": "Impressão de tabelas em formatos pdf e html em Quarto documents",
    "section": "",
    "text": "O que é a impressão de tabelas gtsummary\n\nÉ a etapa onde é gerado os documentos em diferentes formatos\nExistem diferentes pacotes auxiliares que são uteis na formatação da tabela durante a impressão em pdf e html.\n\n\n\n Qual o objetivo\n\nGerar a tabela no formato desejado com detalhes específicos definidos pelo usuário.\n\n\n\n De onde vem a demanda\n\nNecessidade de atender padrões exigidos por quem irá publicar o documento.\n\n\n\n Como fazer\n\nPrimeiramente crie um documento quarto usando o menu File&gt;New File&gt;Quarto document&gt;.\nEm seguida, escolha o nome e formato de saída desejado, neste caso HTML.\nApós o documento criado, insira os códigos abaixo e clique em renderizar para testar os formatos de saída.\n\n\n# packages ----------------------------------------------------------------\n\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(quarto)\n\ndata_iris &lt;- iris %&gt;% janitor::clean_names()\n\n#descriptive analysis\ngts_summary_iris &lt;- \n  data_iris %&gt;% \n  tbl_summary(by = species) %&gt;% \n  add_p() %&gt;% \n  add_stat_label() %&gt;% \n  modify_spanning_header(\n    all_stat_cols() ~ \"**Iris Species**\")\n\nEste método irá gerar uma saída da tabela no formato gt.\n\n#as_gt \ngts_summary_iris %&gt;% as_gt()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nIris Species\n\np-value\n1\n\n\nsetosa\nN = 50\nversicolor\nN = 50\nvirginica\nN = 50\n\n\n\n\nsepal_length, Median (Q1, Q3)\n5.00 (4.80, 5.20)\n5.90 (5.60, 6.30)\n6.50 (6.20, 6.90)\n&lt;0.001\n\n\nsepal_width, Median (Q1, Q3)\n3.40 (3.20, 3.70)\n2.80 (2.50, 3.00)\n3.00 (2.80, 3.20)\n&lt;0.001\n\n\npetal_length, Median (Q1, Q3)\n1.50 (1.40, 1.60)\n4.35 (4.00, 4.60)\n5.55 (5.10, 5.90)\n&lt;0.001\n\n\npetal_width, Median (Q1, Q3)\n0.20 (0.20, 0.30)\n1.30 (1.20, 1.50)\n2.00 (1.80, 2.30)\n&lt;0.001\n\n\n\n1\nKruskal-Wallis rank sum test\n\n\n\n\n\n\n\n\nEste método irá gerar uma saída da tabela no formato flex table muito parecida com a apresentada em tela.\n\n#flex table\ngts_summary_iris %&gt;% as_flex_table()\n\n Iris Species Characteristicsetosa  N = 50versicolor  N = 50virginica  N = 50p-value1sepal_length, Median (Q1, Q3)5.00 (4.80, 5.20)5.90 (5.60, 6.30)6.50 (6.20, 6.90)&lt;0.001sepal_width, Median (Q1, Q3)3.40 (3.20, 3.70)2.80 (2.50, 3.00)3.00 (2.80, 3.20)&lt;0.001petal_length, Median (Q1, Q3)1.50 (1.40, 1.60)4.35 (4.00, 4.60)5.55 (5.10, 5.90)&lt;0.001petal_width, Median (Q1, Q3)0.20 (0.20, 0.30)1.30 (1.20, 1.50)2.00 (1.80, 2.30)&lt;0.0011Kruskal-Wallis rank sum test\n\n\nEste método irá gerar uma saída da tabela no formato hux table.\n\n#huxtable\ngts_summary_iris %&gt;% as_hux_table()\n\n\n\nIris Species\n\n\nCharacteristic\nsetosa\nN = 50\nversicolor\nN = 50\nvirginica\nN = 50\np-value\n\n\nsepal_length, Median (Q1, Q3)5.00 (4.80, 5.20)5.90 (5.60, 6.30)6.50 (6.20, 6.90)&lt;0.001\n\nsepal_width, Median (Q1, Q3)3.40 (3.20, 3.70)2.80 (2.50, 3.00)3.00 (2.80, 3.20)&lt;0.001\n\npetal_length, Median (Q1, Q3)1.50 (1.40, 1.60)4.35 (4.00, 4.60)5.55 (5.10, 5.90)&lt;0.001\n\npetal_width, Median (Q1, Q3)0.20 (0.20, 0.30)1.30 (1.20, 1.50)2.00 (1.80, 2.30)&lt;0.001\n\nKruskal-Wallis rank sum test\n\n\n\n\nEste método irá gerar uma saída da tabela no formato xlsx (esta será salvo na pasta onde você gerar o documento)\n\n#xls\ngts_summary_iris %&gt;%  as_hux_xlsx(\n  file = \"table_1.xlsx\")\n\n\n\n Pra onde vai quem é o cliente\n\nRelatórios reproduzíveis em quarto documents.\nTabelas para análises e publicações futuras em projetos de pesquisa\n\n\n\n Qual o resultado\n\nFacilita a construção de tabela e melhor comunicação dos dados.\nPossibilia a reprodução rápida e prática da tabela.\nMelhora a consistência na apresentação de dados.\n\nVídeo tema para este post"
  },
  {
    "objectID": "posts/pca-explore/pca-explore.html",
    "href": "posts/pca-explore/pca-explore.html",
    "title": "Interpretando Análise de componentes principais PCA",
    "section": "",
    "text": "O que é a interpretação de Análise de Componentes Principais PCA\n\nEnvolve compreender como os dados originais são transformados e representados em um novo espaço de dimensões reduzidas.\nVerificar a variância explicada de cada componente principal Lê, and Husson (2008)\nAnalisar os componentes principais como loadings, scores, rotations, coordenadas, cos2 entre outros elementos Kassambara (n.d.)\n\n\n\n Qual o objetivo\n\nCompreender as principais características dos dados representadas pelas direções de maior variância na PCA.\nCompreender a existência de correlações ou seja, se houver duas ou mais variáveis altamente correlacionadas, estas provavelmente serão representadas por um único componente Kuhn and Johnson (2013)\nVerificar a existência de outliers, que é representado na PCA onde um ponto pode estar muito longo do centro da distribuição.\n\n\n\n De onde vem a demanda\n\nNecessidade de resolver o problema da maldição da dimensionalidade que são situações onde o número de variáveis(dimensões) é muito grande, e o volume de dados necessários para obter estimativas precisas também aumenta exponencialmente. Isso pode levar a problemas de superajuste e dificuldade de encontrar padrões significativos nos dados.\nDificuldade de encontrar variáveis mais importantes o que pode levar a problemas no ajuste e instabilidades em modelos de predição\nNecessidade de reduzir ruído e variabilidade nos dados os quais podem afetar na precisão dos modelos estatísticos e machine learning.\nNecessidade de otimizar o armazenamento e processamento dos dados, principalmente em situações onde os conjuntos de dados são muito grandes Spiegelhalter (2021)\nNecessidade de melhorar a visualização de dados facilitando a compreensão dos padrões e relações entre as variáveis.\n\n\n\n Como fazer\nOs pacotes e conjunto de dados abaixo são necessários para fazer o exercício. Basta instalá-los, ou se já tiver eles instalados em seu ambiente R, é só chamá-los usando a função library\n\n# pacotes\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(tidymodels)\n\n#dados\ndata(iris)\niris &lt;- iris %&gt;% janitor::clean_names()\n\nPCA usando a função prcomp da base R\n\n#pca_prcomp\npca_iris_prc &lt;- iris %&gt;% select(-species) %&gt;% prcomp(center = TRUE, scale. = TRUE)\n\n#plot\nfviz_pca_var(pca_iris_prc)\n\n\n\n\n\n\n\n#pca_result_prcomp\nloadings_prc &lt;- pca_iris_prc$rotation\nscores_prc   &lt;- pca_iris_prc$x\nvariance_prc &lt;- (pca_iris_prc$sdev)^2\n\nPCA usando o pacote factomineRe factoextra\n\n#model pca\npca_iris_fcm &lt;- iris %&gt;% PCA(scale.unit = TRUE, quali.sup = 5, graph = TRUE )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#pca_result_factominer explore\nloadings_fcm  &lt;- sweep(pca_iris_fcm$var$coord,2,sqrt(pca_iris_fcm$eig[,1]), FUN = \"/\")\nscores_fcm   &lt;- pca_iris_fcm$ind$coord\nvariance_fcm &lt;- pca_iris_fcm$eig\n\n#plot\nfviz_pca_var(pca_iris_fcm)\n\n\n\n\n\n\n\n\nPCA usando o pacote tidymodels\n\n#pca tidymodels\npca_iris_tdm &lt;- \n  iris %&gt;% \n  recipe(species~.) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_pca(all_numeric_predictors()) %&gt;% \n  prep()\n\nsummary(pca_iris_tdm$steps)\n\n     Length Class          Mode\n[1,]  9     step_normalize list\n[2,] 13     step_pca       list\n\n#pca_result_tidymodels\nloadings_tdm &lt;- pca_iris_tdm$steps[[2]]$res$rotation\nscores_tdm   &lt;- bake(pca_iris_tdm, new_data = NULL)\nvariance_tdm &lt;- (pca_iris_tdm$steps[[2]]$res$sdev)^2\n\n#plot\ndata_plot &lt;-  tidy(pca_iris_tdm,2)\n\ndata_plot %&gt;% \n  filter(component %in% paste0(\"PC\", 1:2)) %&gt;% \n  ggplot(aes(x=value, y=terms, fill = terms))+\n  geom_col()+\n  facet_grid(~component, scales = \"free\")+\n  scale_fill_brewer(palette = \"BrBG\")\n\n\n\n\n\n\n\n\nConclusão usando os diferentes tipos de pacotes e funções e responendo a questão. Será que há diferença nos resultados quando uso prcomp, factomineR ou tidymodels ?\nLoadings usando os diferentes pacotes sendo prc = prcomp, fcm = factominer e tdm = tidymodels\n\n#conclusão\nloadings_prc %&gt;% head()\n\n                    PC1         PC2        PC3        PC4\nsepal_length  0.5210659 -0.37741762  0.7195664  0.2612863\nsepal_width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\npetal_length  0.5804131 -0.02449161 -0.1421264 -0.8014492\npetal_width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\nloadings_fcm %&gt;% head()\n\n                  Dim.1      Dim.2      Dim.3      Dim.4\nsepal_length  0.5210659 0.37741762 -0.7195664 -0.2612863\nsepal_width  -0.2693474 0.92329566  0.2443818  0.1235096\npetal_length  0.5804131 0.02449161  0.1421264  0.8014492\npetal_width   0.5648565 0.06694199  0.6342727 -0.5235971\n\nloadings_tdm %&gt;% head()\n\n                    PC1         PC2        PC3        PC4\nsepal_length  0.5210659 -0.37741762  0.7195664  0.2612863\nsepal_width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\npetal_length  0.5804131 -0.02449161 -0.1421264 -0.8014492\npetal_width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n\nScores usando os diferentes pacotes sendo prc = prcomp, fcm = factominer e tdm = tidymodels\n\nscores_prc   %&gt;% head()\n\n           PC1        PC2         PC3          PC4\n[1,] -2.257141 -0.4784238  0.12727962  0.024087508\n[2,] -2.074013  0.6718827  0.23382552  0.102662845\n[3,] -2.356335  0.3407664 -0.04405390  0.028282305\n[4,] -2.291707  0.5953999 -0.09098530 -0.065735340\n[5,] -2.381863 -0.6446757 -0.01568565 -0.035802870\n[6,] -2.068701 -1.4842053 -0.02687825  0.006586116\n\nscores_fcm   %&gt;% head()\n\n      Dim.1      Dim.2       Dim.3       Dim.4\n1 -2.264703  0.4800266 -0.12770602 -0.02416820\n2 -2.080961 -0.6741336 -0.23460885 -0.10300677\n3 -2.364229 -0.3419080  0.04420148 -0.02837705\n4 -2.299384 -0.5973945  0.09129011  0.06595556\n5 -2.389842  0.6468354  0.01573820  0.03592281\n6 -2.075631  1.4891775  0.02696829 -0.00660818\n\nscores_tdm   %&gt;% head()\n\n# A tibble: 6 × 5\n  species   PC1    PC2     PC3      PC4\n  &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 setosa  -2.26 -0.478  0.127   0.0241 \n2 setosa  -2.07  0.672  0.234   0.103  \n3 setosa  -2.36  0.341 -0.0441  0.0283 \n4 setosa  -2.29  0.595 -0.0910 -0.0657 \n5 setosa  -2.38 -0.645 -0.0157 -0.0358 \n6 setosa  -2.07 -1.48  -0.0269  0.00659\n\n\nVariância usando os diferentes pacotes sendo prc = prcomp, fcm = factominer e tdm = tidymodels\n\nvariance_prc %&gt;% head()\n\n[1] 2.91849782 0.91403047 0.14675688 0.02071484\n\nvariance_fcm %&gt;% head()\n\n       eigenvalue percentage of variance cumulative percentage of variance\ncomp 1 2.91849782             72.9624454                          72.96245\ncomp 2 0.91403047             22.8507618                          95.81321\ncomp 3 0.14675688              3.6689219                          99.48213\ncomp 4 0.02071484              0.5178709                         100.00000\n\nvariance_tdm %&gt;% head()\n\n[1] 2.91849782 0.91403047 0.14675688 0.02071484\n\n\n\n\n Pra onde vai quem é o cliente\n\nSão utilizados como suporte em documentos técnicos e científicos.\nSão utilizados como variáveis substitutas não correlacionadas em modelos machine learning.\n\n\n\n Qual o resultado\n\nFacilita a análise, compreensão, explicação e comunicação dos dados.\nPossibilia a compreensão melhor dos dados auxiliando na solução de um problema prático ou científico.\nMelhora a consistência na apresentação de dados.\n\nVídeo tema para este post\n\n\n\n\n\n\n\nReferences\n\nKassambara, Alboukadel. n.d. Practical Guide To Principal Component Methods in R.\n\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive Modeling. 2013ª edição. New York: Springer.\n\n\nLê, Josse, and F Husson. 2008. “Lê, s., Josse, j. & Husson, f. (2008). FactoMineR: An r Package for Multivariate Analysis. Journal of Statistical Software. 25(1). Pp. 1-18.” http://factominer.free.fr/factomethods/principal-components-analysis.html.\n\n\nSpiegelhalter, David. 2021. The Art of Statistics: How to Learn from Data. Basic Books."
  },
  {
    "objectID": "posts/python-plots/py-plots.html",
    "href": "posts/python-plots/py-plots.html",
    "title": "Visualização de dados usando Python",
    "section": "",
    "text": "O que é a visualização de dados\n\nÉ uma etapa importante no processo de ciência dos dados onde o cientista busca por determinados padrões, distribuição, tendências ou formatos específicos os quais fornecem informações relevantes para as próximas etapas durante uma análise ou avaliação de seus resultados.\n\n\n\n Qual o objetivo\n\nDar ao cientísta uma perspectiva diferente de interação com os dados que está analisando. Aumentando as possibilidades na identificação de características as quais não seriam possíveis usando outros formatos de apresentação. Isso permite traçar cenários, hipóteses e possíveis relações de causa-e-efeito.\n\n\n\n De onde vem a demanda\n\nVem da necessidade de obter um formato de apresentação de dados que seja compreendido pelo cerebro humano de maneira mais eficiência e rápida.\n\n\n\n Como fazer\nPrimeiro precisamos importar as bibliotecas do Python. Neste caso importaremos inicialmente as bibliotecas pandas e numpy que são usadas para manipulação de dados assim como tidyverse, dplyr e tidyr no R.\nTambém faremos a importação dos dados iris que já esta armazenado na biblioteca sklearn\n\nfrom sklearn import datasets\nimport pandas as pd\nimport numpy as np\n\ndata_iris = datasets.load_iris() #get data\n\nO próximo passo é converter o dataset em um pandas dataframe. Para quem vem do R, o pandas é um pacote similar ao tidyverse e SQL que facilita o manuseio, tratamento e manipulação de dados.\n\ndf_iris = pd.DataFrame(\n    data = np.c_[data_iris[\"data\"], data_iris[\"target\"]],\n    columns= data_iris[\"feature_names\"] + [\"target\"]\n    )\n\nVamos verificar alguns registros do conjunto de dados usando pandas.\n\ndf_iris.head(5)\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0.0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0.0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0.0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0.0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0.0\n\n\n\n\n\n\n\nVamos criar uma variável categórica armazenando as classes do dataseet que são setosa, versicolor e virginica* e \n\nspecies = []\n\nfor i in range(len(df_iris[\"target\"])):\n    if df_iris[\"target\"][i] == 0:\n        species.append(\"setosa\")\n    elif df_iris[\"target\"][i] == 1:\n        species.append(\"versicolor\")\n    else:\n        species.append(\"virginica\")\n\ndf_iris[\"species\"] = species\n\nVamos fazer uma análise descritiva.\n\ndf_iris.groupby('species').size()\n\ndf_iris.describe()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n1.000000\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n0.819232\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n0.000000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n0.000000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n1.000000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n2.000000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n2.000000\n\n\n\n\n\n\n\nVamos plotar um gráfico de correlação do conjunto de dados usando a biblioteca matplotlib\n\nimport matplotlib.pyplot as plt\n\ndf_iris.head(5)\n\ndf_iris.columns\n\ndf_iris.plot(\n    kind = \"scatter\",\n    x = 'sepal length (cm)',\n    y = 'sepal width (cm)')\n\n\n\n\n\n\n\n\nVamos plotar um gráfico semelhante ao anterior usando a biblioteca seaborn\n\nimport seaborn as sns\n\nsns_iris = sns.load_dataset(\"iris\")\n\nsns.pairplot(sns_iris, hue = \"species\")\n\n\n\n\n\n\n\n\nAgora vamos fazer o mesmo usando a biblioteca plotnine. Esta biblioteca foi desenvolvida usando como base o pacote ggplot2 o que pode facilitar a interpretação do código e também a construção de gráficos mais complexos usando o Python.\n\nfrom plotnine import ggplot, aes, geom_point\n\n(\n    ggplot(sns_iris)\n    + aes(x = 'sepal_length', y = 'petal_length', fill = 'species')\n    + geom_point()\n)\n\n\n\n\n\n\n\n\nAgora vamos construir um modelo machine learning usando a biblioteca sklearn usando o conjunto iris dataset para facilitar o entendimento.\nChamaremos a biblioteca, construiremos a matriz, e faremos o split de dados em treino e teste.\n\nfrom sklearn.model_selection import train_test_split\n\n#design matrix\nX = df_iris.drop([\"target\", \"species\"], axis = 1)\n\n#converting a numpy array petal length petal width\nX = X.to_numpy()[:, (2,3)]\ny = df_iris[\"target\"]\n\n#split  into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.5, random_state=42)\n\nVamos especificar o modelo a ser usado, neste caso vamos usar regressão logística.\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\nVamos fazer a previsão usando o conjunto de treino\n\nmdl_pred_logreg_iris = log_reg.predict(X_train)\n\nmdl_pred_logreg_iris\n\narray([1., 2., 1., 0., 1., 2., 0., 0., 1., 2., 0., 2., 0., 0., 2., 1., 2.,\n       2., 2., 2., 1., 0., 0., 1., 2., 0., 0., 0., 1., 2., 0., 2., 2., 0.,\n       1., 1., 2., 1., 2., 0., 2., 1., 2., 1., 1., 1., 0., 1., 1., 0., 1.,\n       2., 2., 0., 1., 2., 2., 0., 2., 0., 1., 2., 2., 1., 2., 1., 1., 2.,\n       2., 0., 1., 1., 0., 1., 2.])\n\n\nVamos fazer a previsão usando o conjunto de teste\n\ntes_prediction = log_reg.predict(X_test)\n\ntes_prediction\n\narray([1., 0., 2., 1., 1., 0., 1., 2., 1., 1., 2., 0., 0., 0., 0., 1., 2.,\n       1., 1., 2., 0., 2., 0., 2., 2., 2., 2., 2., 0., 0., 0., 0., 1., 0.,\n       0., 2., 1., 0., 0., 0., 2., 1., 1., 0., 0., 1., 2., 2., 1., 2., 1.,\n       2., 1., 0., 2., 1., 0., 0., 0., 1., 2., 0., 0., 0., 1., 0., 1., 2.,\n       0., 1., 2., 0., 2., 2., 1.])\n\n\nVamos avaliar a performance do modelo usando o conjunto de treino\n\nfrom sklearn import metrics\n\nprint(\"Precision, Recall, Confusion Matrix, in training\\n\")\n\nprint(metrics.classification_report(y_train, mdl_pred_logreg_iris, digits = 3))\n\nprint(metrics.confusion_matrix(y_train, mdl_pred_logreg_iris))\n\nPrecision, Recall, Confusion Matrix, in training\n\n              precision    recall  f1-score   support\n\n         0.0      1.000     1.000     1.000        21\n         1.0      0.923     0.889     0.906        27\n         2.0      0.893     0.926     0.909        27\n\n    accuracy                          0.933        75\n   macro avg      0.939     0.938     0.938        75\nweighted avg      0.934     0.933     0.933        75\n\n[[21  0  0]\n [ 0 24  3]\n [ 0  2 25]]\n\n\nVamos avaliar a performance do modelo usando o conjunto de teste\n\nprint(\"Precision, Recall, Confusion Matrix, in testing\\n\")\n\nprint(metrics.classification_report(y_test, tes_prediction, digits = 3))\n\n#confusion matrix\nprint(metrics.confusion_matrix(y_test, tes_prediction))\n\nPrecision, Recall, Confusion Matrix, in testing\n\n              precision    recall  f1-score   support\n\n         0.0      1.000     1.000     1.000        29\n         1.0      1.000     1.000     1.000        23\n         2.0      1.000     1.000     1.000        23\n\n    accuracy                          1.000        75\n   macro avg      1.000     1.000     1.000        75\nweighted avg      1.000     1.000     1.000        75\n\n[[29  0  0]\n [ 0 23  0]\n [ 0  0 23]]\n\n\n\n\n Pra onde vai quem é o cliente\n\nSe for nas fases iniciais de exploração o próximo passo é a construção de hipoteses e cenários.\nSe for nas fases de validações e análise de resultados, a próxima etapa seria a construção de conclusões.\n\n\n\n Qual o resultado\n\nFacilita a análise, compreensão, explicação e comunicação dos dados.\nPossibilia que a análise dos dados sejam vistos com outra perspectiva.\nMelhora a consistência na apresentação de dados.\n\nVídeo tema para este post"
  },
  {
    "objectID": "posts/two_sigma_problem/two_sigma_problem.html",
    "href": "posts/two_sigma_problem/two_sigma_problem.html",
    "title": "Problema de 2 sigma de Bloom",
    "section": "",
    "text": "Imagine que seu filho está numa sala de aula com 30 alunos. Todos aprendem com o mesmo professor, no mesmo ritmo, com o mesmo material. Isso é o ensino tradicional.\nAgora imagine que seu filho tivesse um tutor particular, alguém que ensinasse só para ele, no ritmo dele, tirando dúvidas, reforçando o que ele não entendeu, e adaptando o conteúdo às necessidades dele. O que você acha que aconteceria com o desempenho dele?\nFoi exatamente isso que o pesquisador Benjamin Bloom @bloom1984 descobriu. Alunos que recebem ensino individualizado com tutoria têm um desempenho muito melhor, cerca de 2 desvios padrão acima dos alunos do ensino tradicional. Isso é o que ele chamou de efeito 2 sigma.\nO que significa “2 sigma”?\nSem entrar muito na matemática, “2 sigma” quer dizer que esses alunos com tutoria aprendem melhor do que 98% dos alunos que estão no ensino tradicional. É uma diferença enorme!\n\n\n\n\n\n\n\n\n\n\n\ntipo_aprendizadomediadesvio_padrao\n\nPequenos Grupos69.67.74\n\nTradicional60.99.13\n\nTutoria75.54.75\n\n\n\n\n\n\n.y.group1group2effsizen1n2magnitude\n\npontuacaoTradicionalPequenos Grupos-1.03100100large\n\npontuacaoTradicionalTutoria-2.01100100large\n\n\n\n\nPor que isso é um problema?\nPorque, apesar de sabermos que a tutoria personalizada funciona muito bem, ela é cara e difícil de aplicar em larga escala. Não dá para colocar um tutor para cada aluno em todas as escolas.\nEntão o “problema de 2 sigma” é:\nComo podemos fazer com que todos os alunos aprendam tão bem quanto os que têm tutores, mas sem precisar de um tutor para cada um?\nPor que isso é importante?\nEsse problema é super relevante porque mostra que todos os alunos podem aprender muito mais, se forem ensinados da forma certa. Incentiva a busca por novas formas de ensino, como o uso de tecnologia, ensino adaptativo, inteligência artificial, etc. Ajuda pais e educadores a entenderem que o potencial dos alunos é muito maior do que parece no ensino tradicional."
  },
  {
    "objectID": "posts/two_sigma_problem/two_sigma_problem.html#o-que-é-o-problema-de-2-sigma-de-bloom",
    "href": "posts/two_sigma_problem/two_sigma_problem.html#o-que-é-o-problema-de-2-sigma-de-bloom",
    "title": "Problema de 2 sigma de Bloom",
    "section": "",
    "text": "Imagine que seu filho está numa sala de aula com 30 alunos. Todos aprendem com o mesmo professor, no mesmo ritmo, com o mesmo material. Isso é o ensino tradicional.\nAgora imagine que seu filho tivesse um tutor particular, alguém que ensinasse só para ele, no ritmo dele, tirando dúvidas, reforçando o que ele não entendeu, e adaptando o conteúdo às necessidades dele. O que você acha que aconteceria com o desempenho dele?\nFoi exatamente isso que o pesquisador Benjamin Bloom @bloom1984 descobriu. Alunos que recebem ensino individualizado com tutoria têm um desempenho muito melhor, cerca de 2 desvios padrão acima dos alunos do ensino tradicional. Isso é o que ele chamou de efeito 2 sigma.\nO que significa “2 sigma”?\nSem entrar muito na matemática, “2 sigma” quer dizer que esses alunos com tutoria aprendem melhor do que 98% dos alunos que estão no ensino tradicional. É uma diferença enorme!\n\n\n\n\n\n\n\n\n\n\n\ntipo_aprendizadomediadesvio_padrao\n\nPequenos Grupos69.67.74\n\nTradicional60.99.13\n\nTutoria75.54.75\n\n\n\n\n\n\n.y.group1group2effsizen1n2magnitude\n\npontuacaoTradicionalPequenos Grupos-1.03100100large\n\npontuacaoTradicionalTutoria-2.01100100large\n\n\n\n\nPor que isso é um problema?\nPorque, apesar de sabermos que a tutoria personalizada funciona muito bem, ela é cara e difícil de aplicar em larga escala. Não dá para colocar um tutor para cada aluno em todas as escolas.\nEntão o “problema de 2 sigma” é:\nComo podemos fazer com que todos os alunos aprendam tão bem quanto os que têm tutores, mas sem precisar de um tutor para cada um?\nPor que isso é importante?\nEsse problema é super relevante porque mostra que todos os alunos podem aprender muito mais, se forem ensinados da forma certa. Incentiva a busca por novas formas de ensino, como o uso de tecnologia, ensino adaptativo, inteligência artificial, etc. Ajuda pais e educadores a entenderem que o potencial dos alunos é muito maior do que parece no ensino tradicional."
  },
  {
    "objectID": "posts/two_sigma_problem/two_sigma_problem.html#o-que-mais-bloom-descobriu",
    "href": "posts/two_sigma_problem/two_sigma_problem.html#o-que-mais-bloom-descobriu",
    "title": "Problema de 2 sigma de Bloom",
    "section": "O que mais Bloom descobriu?",
    "text": "O que mais Bloom descobriu?\n\nVocê mencionou que além do resultado, a tutorial individualizada mostrada no trabalho de Benjamin Bloom, houve também melhorias significativas em tempo de engajamento, atitudes dos alunos e redução da correlação entre aptidão inicial e desempenho final, especialmente no grupo de tutoria. Me explique melhor cada um desses componentes.\n\nEsses indicadores que Bloom menciona são fundamentais para entender não apenas o desempenho acadêmico, mas também a qualidade da experiência de aprendizagem. Vamos detalhar cada um deles com exemplos práticos:\n1. Tempo de Engajamento É o tempo efetivo que o aluno passa envolvido ativamente com a aprendizagem — seja ouvindo, lendo, resolvendo problemas, discutindo ou praticando. Por que é importante? Quanto maior o tempo de engajamento, maior a probabilidade de o aluno compreender e reter o conteúdoExemplo prático: em uma aula tradicional, alguns alunos podem se distrair ou ficar passivos. Já em tutoria, o aluno está constantemente envolvido: o tutor faz perguntas, propõe desafios, ajusta o ritmo. Resultado: o aluno passa mais tempo aprendendo de fato, o que contribui para melhores resultados.\n2. Atitudes dos Alunos Refere-se à motivação, interesse, confiança e satisfação dos alunos em relação à aprendizagem. Por que é importante? Alunos com atitudes positivas tendem a persistir diante de dificuldades, buscar ajuda, e se envolver mais profundamente com o conteúdo. Exemplo prático: Um aluno que recebe atenção individualizada sente que está progredindo. Isso gera confiança (“eu consigo aprender!”) e motivação (“quero continuar aprendendo!”). Em contraste, alunos em ambientes convencionais podem se sentir perdidos ou desmotivados se não acompanham o ritmo da turma.\n3. Redução da Correlação entre Aptidão Inicial e Desempenho Final Normalmente, há uma correlação entre a aptidão inicial (conhecimentos prévios, habilidades cognitivas) e o desempenho final (nota ou domínio do conteúdo). Por que é importante? Em ambientes convencionais, alunos com maior aptidão inicial tendem a ter melhor desempenho. Isso amplia desigualdades. Exemplo prático:Na tutoria, o ensino é adaptado ao ritmo e às necessidades de cada aluno. Isso permite que alunos com menor aptidão inicial também atinjam altos níveis de desempenho. Resultado: a correlação entre “o que o aluno sabia antes” e “o que ele aprendeu” diminui, mostrando que todos podem aprender bem com apoio adequado."
  },
  {
    "objectID": "posts/two_sigma_problem/two_sigma_problem.html#abordagem-de-layton---pré-requisitos-cognitivos",
    "href": "posts/two_sigma_problem/two_sigma_problem.html#abordagem-de-layton---pré-requisitos-cognitivos",
    "title": "Problema de 2 sigma de Bloom",
    "section": "Abordagem de Layton - Pré-Requisitos Cognitivos",
    "text": "Abordagem de Layton - Pré-Requisitos Cognitivos\n\nConsidere você um especialista no problema de 2 sigma de Bloom. Em especial no artigo publicado por ele em 1984 em Chicago. Layton sugeriu que uma abordagem para o problema de 2 sigma seria usar o aprendizado de domínio durante o curso avançado em uma sequência, mas, além disso, tentar melhorar os pré-requisitos de entrada cognitiva inicial dos alunos no começo do curso. Me explique melhor essa abordagem e se possível me mostre com exemplos práticos.\n\nA abordagem sugerida por Layton @soto1983 para enfrentar o problema de 2 sigma de Bloom envolve duas estratégias principais:\n\nMelhorar os pré-requisitos cognitivos dos alunos antes do início do curso.\nUtilizar o aprendizado de domínio (mastery learning) em sequência ao longo do curso avançado.\n\nVamos entender cada uma delas com mais profundidade e exemplos práticos.\n1. Melhoria dos Pré-Requisitos Cognitivos\nBloom identificou que os características de entrada dos alunos — como conhecimentos prévios, habilidades cognitivas e atitudes — têm grande impacto no desempenho acadêmico 1. Layton propôs que, antes de iniciar um curso avançado, os alunos deveriam passar por uma fase preparatória que fortaleça essas características.Exemplos práticos: - Curso de Física Avançada: Antes de iniciar o curso, os alunos fazem um módulo introdutório sobre álgebra vetorial e cinemática básica, com foco em resolução de problemas. - Curso de Programação em Python: Os alunos passam por um minicurso sobre lógica de programação e estruturas de dados básicas, com exercícios práticos e feedback imediato.\nEssa preparação reduz a variabilidade entre os alunos e aumenta a chance de todos acompanharem o conteúdo avançado com sucesso.\n2. Aprendizado de Domínio (Mastery Learning) em Sequência\nO aprendizado de domínio é uma abordagem em que os alunos só avançam para o próximo conteúdo após dominar completamente o anterior. Bloom mostrou que isso pode levar a um ganho de até 1 sigma em desempenho1. Layton sugeriu aplicar essa técnica em sequência, ou seja, ao longo de todo o curso, com ciclos de:\n\n\n\n\nflowchart LR\n    A[Ensino] --&gt; B[Avaliação formativa]\n    B --&gt; C[Feedback e correção]\n    C --&gt; D[Nova avaliação] \n\n\n\nFluxo simples de aprendizado\n\n\n\nEsse diagrama representa um ciclo contínuo de aprendizagem, onde o ensino leva à avaliação formativa, que por sua vez gera feedback e correções, culminando em uma nova avaliação para verificar o progresso\nExemplo prático no curso de Matemática: - Módulo 1: Álgebra — os alunos fazem exercícios e testes formativos. - Se não dominarem, recebem atividades corretivas e refazem o teste. - Só então avançam para o Módulo 2: Funções.\nEsse ciclo contínuo garante que todos os alunos estejam prontos para os próximos desafios, reduzindo lacunas de aprendizagem.\nCombinação das Estratégias\nA proposta de Layton é que a combinação dessas duas abordagens — entrada cognitiva fortalecida + aprendizado de domínio — pode aproximar o desempenho dos alunos ao nível da tutoria individual, que é o cerne do problema de 2 sigma. Bloom e seus colegas chegaram a testar combinações de variáveis alteráveis (como feedback, tempo de estudo, ambiente familiar) e observaram que algumas combinações se aproximam do efeito de 2 sigma 1.\nImpacto Esperado\n\nRedução da variabilidade entre alunos.\nMaior engajamento e tempo em tarefa.\nDesempenho médio significativamente superior ao ensino convencional.\n\nE qual o impacto observado?\n\nNo artigo, diz também que mais importante foi a melhora no desempenho das turmas “aprimoradas” em relação às outras duas turmas no primeiro teste formativo do curso avançado. As duas turmas aprimoradas, que haviam sido auxiliadas nos pré-requisitos iniciais, apresentaram aproximadamente 0.7 sigma a mais do que as outras duas turmas no primeiro teste formativo aplicado ao final de um período de aprendizado de duas semanas no curso avançado. Me fale mais sobre esses resultados.\n\nO trecho que você mencionou sobre o desempenho das turmas aprimoradas no primeiro teste formativo é um dos pontos mais reveladores do artigo de Bloom (1984) sobre o problema de 2 sigma.\nO Experimento e os Resultados\nNo estudo, Bloom e seus colegas compararam quatro grupos de alunos em um curso avançado. Dois desses grupos receberam intervenções para melhorar os pré-requisitos cognitivos antes do início do curso, enquanto os outros dois não.\nApós duas semanas de instrução, todos os grupos realizaram um teste formativo. Os resultados mostraram que: - As duas turmas aprimoradas(com reforço nos pré-requisitos) tiveram um desempenho 0.7 sigma acima das turmas que não receberam essa preparação 1. - Esse ganho foi observado logo no primeiro teste, indicando que a intervenção nos pré-requisitos teve um impacto imediato e significativo.\nInterpretação dos Resultados Esse resultado é importante por vários motivos:\n\nValidação da hipótese de Layton: A ideia de que melhorar os pré-requisitos cognitivos pode elevar o desempenho inicial dos alunos foi confirmada empiricamente.\nRedução da variabilidade: Ao nivelar os conhecimentos iniciais, os alunos começaram o curso com uma base mais homogênea, o que facilita o ensino em grupo.\nPotencial para escalabilidade: Diferente da tutoria individual, essa abordagem pode ser aplicada em turmas maiores com custo relativamente baixo.\n\n\n\n\n\n\n\n\n\nCondições de aprendizado utilizados na pesquisa\n\n\nTipo aprendizado\nDetalhes\n\n\n\n\nConvencional\nTodos os alunos aprendem o mesmo conteúdo, ao mesmo tempo, com o mesmo ritmo. A aula segue um cronograma fixo, e as provas acontecem em datas marcadas, independentemente de o aluno ter dominado o conteúdo ou não\n\n\nAprendizagem por Domínio\nNinguém avança para a próxima etapa até dominar bem a anterior. Se o aluno ainda não entendeu bem a matéria, ele recebe mais apoio, mais tempo e atividades extras até que esteja pronto para seguir em frente.\n\n\nPré-requisitos Reforçados\nAntes de ensinar um novo conteúdo, o professor reforça os conhecimentos básicos que são essenciais para entender o que vem a seguir. É como revisar os alicerces antes de construir o próximo andar.\n\n\nTutoria Individual\nO aluno recebe atenção personalizada de um tutor (professor ou colega mais experiente), que adapta o ensino às suas necessidades específicas.\n\n\n\n\n\n\n\nO gráfico abaixo demonstra os resultados dos estudos de @anania1981 e @burke1983 citados por Bloom em sua puplicação.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpanda para saber o que são pré-requisitos cognitivos\n\n\n\n\n\nO que são os “pré-requisitos cognitivos”?\nSão conhecimentos e habilidades que os alunos precisam ter antes de iniciar um curso avançado. Podem incluir:\n\nConceitos fundamentais da disciplina.\nEstratégias de resolução de problemas.\nHabilidades metacognitivas (como autorregulação e monitoramento do próprio aprendizado).\n\nExemplo prático aplicado\nImagine um curso de Cálculo Diferencial e Integral. Antes de começar, os alunos passam por:\n\nUm módulo de revisão de funções, álgebra e trigonometria.\nAtividades diagnósticas com feedback imediato.\nExercícios de raciocínio lógico e visualização gráfica.\n\nApós duas semanas, os alunos fazem um teste sobre limites e derivadas. Os que passaram pela preparação têm desempenho significativamente superior — como no estudo de Bloom.\nIntervenções para melhorar os pré-requisitos cognitivos\nSão estratégias pedagógicas e terapêuticas que visam desenvolver habilidades mentais fundamentais para a aprendizagem, como atenção, memória, linguagem, percepção e raciocínio. Esses pré-requisitos são essenciais para que os alunos possam compreender, processar e aplicar o conteúdo escolar de forma eficaz. Essas intervenções podem ser aplicadas por professores, psicopedagogos, neuropsicólogos ou terapeutas ocupacionais, dependendo da necessidade e do contexto.\nQuais tipos de pré-requisitos cognitivos?\nAlgumas das habilidades mentais básicas que sustentam o aprendizado, são: - Atenção (focal, sustentada, seletiva) Jogos de atenção seletiva (ex: encontrar diferenças entre figuras), Atividades com tempo limitado para manter foco, Técnicas de mindfulness adaptadas para crianças\n\nMemória (curto e longo prazo, de trabalho), Jogos de memória (cartas, sequências), Repetição espaçada de conteúdos,Uso de mnemônicos e mapas mentais\nPercepção visual e auditiva, Atividades de discriminação de sons e letras, Exercícios de coordenação visomotora (ligar pontos, copiar figuras), Treinamento auditivo com rimas e padrões sonoros\nLinguagem oral e escrita Contação de histórias com reconto, Jogos de vocabulário e categorização, Atividades de consciência fonológica (rimas, sílabas),\nRaciocínio lógico e resolução de problemas Problemas matemáticos contextualizados, Jogos de tabuleiro e quebra-cabeças, Sequências lógicas e padrões\nFunções executivas (planejamento, organização, autocontrole), Planejamento de tarefas com checklists, Jogos que exigem controle inibitório (ex: “Simon diz”), Atividades que envolvem tomada de decisão."
  },
  {
    "objectID": "posts/two_sigma_problem/two_sigma_problem.html#três-elementos-do-aprendizado-contínuo",
    "href": "posts/two_sigma_problem/two_sigma_problem.html#três-elementos-do-aprendizado-contínuo",
    "title": "Problema de 2 sigma de Bloom",
    "section": "Três elementos do aprendizado contínuo",
    "text": "Três elementos do aprendizado contínuo\n\nNo artigo, ele também cita que eles esperam que esta abordagem de Leyton para o problema 2 sigma seja considerada uma solução amplamente aplicável e econômica, disponível à maioria dos professores que desejam melhorar a aprendizagem dos alunos, o autoconceito acadêmico dos alunos, bem como as atitudes e o interesse dos alunos no aprendizado. Nesse contexto o que seria aprendizagem dos alunos, o autoconceito acadêmico dos alunos, bem como as atitudes e o interesse dos alunos no aprendizado, quais suas características e diferenças entre eles.\n\n1. Aprendizagem dos Alunos\nÉ o ganho real de conhecimento, habilidades e competências que os alunos desenvolvem ao longo do processo educacional, exemplos:\n\nDomínio de conteúdos (fatos, conceitos, procedimentos).\nCapacidade de aplicar o conhecimento em novos contextos.\nDesenvolvimento de habilidades cognitivas como análise, síntese e avaliação.\n\nCaracterísticas: - Mensurável por meio de avaliações (formativas e somativas). Avaliações formativas são realizadas durante o processo de aprendizagem para monitorar o progresso e ajustar o ensino; já as somativas ocorrem ao final de um ciclo para medir o desempenho total do aluno. A formativa tem função diagnóstica, enquanto a somativa é classificatória. - Pode ser influenciada por métodos de ensino, tempo de estudo, feedback, entre outros. - É o foco central de qualquer intervenção educacional.\n2. Autoconceito Acadêmico\nÉ a percepção que o aluno tem de si mesmo como aprendiz. Envolve crenças sobre sua capacidade de aprender, seu valor como estudante e seu desempenho esperado.\nCaracterísticas: - Subjetivo, mas com forte impacto no comportamento e na motivação. - Influenciado por experiências anteriores, feedback de professores, comparação com colegas. - Pode ser positivo (“sou bom em matemática”) ou negativo (“nunca consigo entender química”). - Difere da aprendizagem onde o autoconceito não mede o que o aluno sabe, mas como ele se vê em relação ao aprendizado. - Um aluno pode ter alto desempenho, mas baixo autoconceito (e vice-versa).\n3. Atitudes e Interesse no Aprendizado\nÉ a disposição emocional e motivacional do aluno em relação ao processo de aprender. É a curiosidade e desejo de aprender, persistência diante de dificuldades, valorização do conhecimento como algo útil ou prazeroso.\nCaracterísticas: - Pode ser influenciado por métodos de ensino, relevância do conteúdo, ambiente de sala de aula. - Está ligado à motivação intrínseca (gosto de aprender) e extrínseca (quero boas notas). - Afeta diretamente o engajamento e o tempo dedicado ao estudo.\nRelação entre os três elementos os quais estão interligados\n\nUm aluno com bom autoconceito acadêmico tende a ter atitudes positivas e maior interesse pelo aprendizado.\nIsso, por sua vez, aumenta o engajamento, o que favorece a aprendizagem efetiva.\nIntervenções como a de Layton, que fortalecem os pré-requisitos e usam aprendizado de domínio, melhoram todos esses aspectos simultaneamente."
  },
  {
    "objectID": "posts/two_sigma_problem/two_sigma_problem.html#melhoria-da-leitura",
    "href": "posts/two_sigma_problem/two_sigma_problem.html#melhoria-da-leitura",
    "title": "Problema de 2 sigma de Bloom",
    "section": "Melhoria da leitura",
    "text": "Melhoria da leitura\n\nNo artigo também diz que, há evidências de que alunos que participam de programas especiais para aprimorar sua leitura e/ou seus métodos de estudo e aprendizagem tendem a aprender com mais eficácia. Idealmente, esses programas especiais deveriam estar disponíveis no início de cada novo nível escolar, ou seja, ensino fundamental, ensino médio e assim por diante. Juntando os dados do artigo com estudos atuais da neurociência, existem algumas relações ou evidências nessas descobertas que nos ajude a compreender como melhorar a capacidade de leitura de uma criança?\n\nHá uma forte convergência entre os achados do artigo de Bloom e Layton e os estudos contemporâneos da neurociência sobre leitura infantil\nNeurociência e Leitura Infantil\nA leitura não é uma habilidade natural do cérebro humano — ela exige reorganização neural e ativação de múltiplas áreas cerebrais, como: reconhecimento de letras e palavras (visual), associação entre sons e grafemas que são cada unidade gráfica ou letra(fonológica), compreensão do significado(semântica), movimentos oculares e escrita(motora).\nSegundo estudos recentes 1, o processo de alfabetização envolve o modelo de dupla rota sendo uma rota fonológica (decodificação letra-som) e uma rota lexical (reconhecimento direto de palavras). O desenvolvimento da consciência fonológica é considerado essencial para o sucesso na leitura.\nProgramas Especiais e Intervenções\nO artigo de Bloom sugere que programas de aprimoramento em leitura e métodos de estudo devem ser oferecidos no início de cada novo nível escolar. A neurociência confirma essa ideia: intervenções precoces e sistemáticas podem estimular áreas cerebrais específicas e melhorar significativamente o desempenho em leitura 2.\nEsses programas geralmente incluem exercícios de consciência fonológica como treinamento de memória de trabalho e atenção, leitura guiada com feedback imediato, atividades multissensoriais(visuais, auditivas e táteis).\nConexão com o Problema de 2 Sigma\nA proposta de Layton de melhorar os pré-requisitos cognitivos se alinha com a neurociência ao reconhecer que habilidades básicas como leitura e estudo eficaz são fundamentais para o sucesso acadêmico. Quando essas habilidades são fortalecidas, os alunos aprendem com mais eficácia, desenvolvem maior autoconceito acadêmico, demonstram atitudes mais positivas em relação ao aprendizado.\nImportante\nEsses fatores, combinados, podem gerar ganhos de desempenho próximos ao efeito de 2 sigma, mesmo em ambientes de ensino em grupo.\n\n\n\n\n\n\nExpanda para ver técnicas para melhoria de leitura\n\n\n\n\n\n\nConsciência Fonológica É a capacidade de perceber e manipular os sons da fala (fonemas, sílabas, rimas). É um dos melhores preditores do sucesso na leitura. Sua característica é trabalhar com sons, não com letras, Desenvolve habilidades como segmentação, aliteração, rima, substituição de fonemas (e.g identificar palavras que rimam: “Qual palavra rima com ‘cão’?”, bater palmas para cada sílaba de uma palavra, dizer o som inicial de palavras: “Qual o som inicial de ‘mesa’?”)\nTreinamento de Memória de Trabalho e Atenção Essas funções cognitivas são essenciais para manter informações temporariamente e focar na tarefa. Suas características são melhorar a capacidade de seguir instruções, ajuda na decodificação de palavras e compreensão de texto, trabalha com retenção e manipulação de informações (e.g Repetir sequências de números ou palavras (memória auditiva), jogos como “Simon diz” ou “Repita a sequência”, atividades com distrações controladas para treinar atenção seletiva.)\nLeitura Guiada com Feedback Imediato É uma prática em que o educador acompanha a leitura da criança e oferece correções e incentivos imediatamente. Suas características são foco na fluência e compreensão, corrige erros de pronúncia ou entonação na hora, reforça estratégias de decodificação e inferência (e.g Ler em voz alta com o professor ou tutor, parar para discutir o significado de palavras ou frases, repetir trechos com entonação correta após orientação).\nAtividades Multissensoriais (Visuais, Auditivas e Táteis) Essas atividades envolvem múltimos sentidos para reforçar o aprendizado, especialmente eficazes para crianças com dificuldades de leitura (como dislexia). Suas características são d integrar visão, audição e tato, facilita a memorização e compreensão, estimula diferentes áreas do cérebro simultaneamente (e.g Escrever letras na areia ou com os dedos no ar (tátil), associar sons a imagens (auditivo + visual), usar blocos coloridos para formar palavras (visual + motor)."
  },
  {
    "objectID": "posts/two_sigma_problem/two_sigma_problem.html#abordagem-de-carlos-avalos---testar-combinações",
    "href": "posts/two_sigma_problem/two_sigma_problem.html#abordagem-de-carlos-avalos---testar-combinações",
    "title": "Problema de 2 sigma de Bloom",
    "section": "Abordagem de Carlos Avalos - Testar combinações",
    "text": "Abordagem de Carlos Avalos - Testar combinações\n\nNo artigo de Bloom, também cita na página 7 que Carlos Avalos está trabalhando em um estudo sobre o efeito de recursos organizacionais no material instrucional combinados com o aprimoramento inicial do pré-requisito cognitivo e o feedback de aprendizagem de domínio-procedimentos corretivos. Avalos está planejando um projeto de pesquisa que lhe permitirá determinar os efeitos separados de cada um dos três processos, o efeito de qualquer um dos dois processos e o efeito combinado de todos os três processos. Me explique melhor essa parte de forma simples, por que acredito não entendi muito bem.\n\nO que Avalos está tentando fazer?\nEle quer entender quais elementos realmente ajudam os alunos a aprender melhor. Para isso, ele está estudando três processos diferentes:\n\nRecursos organizacionais no material instrucional Como o conteúdo é apresentado com estrutura clara, sequencial e acessível dos conteúdos(organização, clareza, estrutura).\nAprimoramento inicial dos pré-requisitos cognitivos Treinamento de atenção, memória de trabalho, linguagem e raciocínio antes do início do curso.\nFeedback com procedimentos corretivos (aprendizado de domínio) Avaliações formativas com retorno imediato e ações para corrigir erros antes de seguir adiante.\n\nO objetivo do estudo\nAvalos quer descobrir o efeito de cada processo isoladamente, o efeito de qualquer combinação de dois processos. e o efeito da combinação dos três processos juntos. Essa combinação visa replicar os efeitos da tutoria individual (que gerava ganhos de até 2 sigmas) em ambientes de ensino coletivo, com ganhos significativos de até 0.7 sigma\nExemplo simples para entender Imagine quatro grupos de alunos:\n\nGrupo A: recebe só o material bem organizado.\nGrupo B: recebe só o reforço nos pré-requisitos.\nGrupo C: recebe só o feedback corretivo.\nGrupo D: recebe os três juntos.\n\nAvalos vai comparar os resultados desses grupos para ver:\n\nQual processo tem mais impacto sozinho?\nDois juntos funcionam melhor que um?\nOs três juntos funcionam melhor que todos os outros?\n\nPor que isso é importante?\nPorque se ele descobrir que dois processos já são quase tão eficazes quanto os três, pode-se aplicar uma solução mais econômica e viável para escolas com menos recursos. E se os três juntos forem muito mais eficazes, isso reforça a ideia de que uma abordagem integrada é o caminho ideal para melhorar a aprendizagem."
  },
  {
    "objectID": "posts/two_sigma_problem/two_sigma_problem.html#abordagem-de-john-hattie-visible-learning",
    "href": "posts/two_sigma_problem/two_sigma_problem.html#abordagem-de-john-hattie-visible-learning",
    "title": "Problema de 2 sigma de Bloom",
    "section": "Abordagem de John Hattie Visible learning",
    "text": "Abordagem de John Hattie Visible learning\nJohn Hattie @hattie2023 conduziu uma meta-análise que é um estudo científico que conbina os resultados de várias pesquisas anteriores sobre um mesmo tema para chegar a uma conclusão mais robusta e confiável chamada Visible Learning. Esta foi publicada inicialmente em 2009 onde ele analisou mais de 250 influências sobre o desempenho dos alunos, com base em mais de 800 meta-análises envolvendo milhões de estudantes.\nO que ele mediu? Hattie usou o conceito de effect size (tamanho do efeito), que indica o impacto de uma intervenção no aprendizado. Em geral 0.4 é o ponto de corte e acima disso, o efeito é considerado significativo, ou seja, quanto maior o número, maior o impacto.\nPrincipais fatores com alto effect size\n\nFeedback eficaz (Effect size ≈ 0.70) que é o retorno claro, específico e imediato sobre o desempenho do aluno. Isso ajuda o aluno a entender o que está certo, o que precisa melhorar e como melhorar(e.g. após uma redação, o professor destaca pontos fortes e sugere melhorias específicas, como “use conectivos para ligar ideias”)\nClareza instrucional (Effect size ≈ 0.75) que é a apresentação clara dos objetivos, passos e critérios de sucesso. Isso reduz confusão, aumenta o foco e permite que os alunos saibam o que é esperado(e.g. o professor começa a aula dizendo: “Hoje vamos aprender a multiplicar frações. Ao final, você deverá conseguir resolver 5 problemas sozinho.”)\nAvaliação formativa (Effect size ≈ 0.65) que são avaliações durante o processo de aprendizagem para ajustar o ensino. Isso permite identificar dificuldades em tempo real e adaptar a instrução(e.g. durante uma aula de leitura, o professor faz perguntas para verificar compreensão e ajusta o ritmo conforme necessário.)\nEnvolvimento ativo do aluno (Effect size ≈ 0.61) que é a participação ativa na construção do conhecimento, com autonomia e reflexão. Isso Aumenta o engajamento, a retenção e o senso de responsabilidade(e.g. em vez de apenas ouvir, os alunos discutem, resolvem problemas, criam projetos e explicam conceitos uns aos outros)."
  },
  {
    "objectID": "posts/classificacao_ai/classificacao_ai.html",
    "href": "posts/classificacao_ai/classificacao_ai.html",
    "title": "classificacao_ai",
    "section": "",
    "text": "A classificação da Inteligência Artificial é o processo de organizar os diferentes tipos de sistemas de IA com base em critérios como função, complexidade, autonomia, impacto social e contexto de uso.\nAssim como classificamos tecnologias em categorias distintas (como softwares, veículos ou medicamentos) de acordo com função, complexidade e contexto de uso, podemos aplicar critérios semelhantes à IA, que com uma taxonomia clara, possa ser melhor compreendida e aplicada de forma responsável.\n\n\n\n\n\n\n\n\n\nVeículo (analogia)\nInteligência Artificial (realidade)\n\n\n\n\nPalavra genérica que engloba carros, bicicletas, ônibus, foguetes, etc.\nTermo guarda-chuva que inclui desde chatbots simples até sistemas autônomos complexos.\n\n\nDiscussões confusas sobre “veículos” sem distinguir entre tipos.\nDebates sobre IA (ética, impacto, regulação) sem diferenciar entre modelos e aplicações.\n\n\nAvanços em foguetes confundidos com melhorias em carros.\nAvanços em IA generativa confundidos com IA de recomendação ou automação industrial.\n\n\nGolpes e desinformação prosperam na ambiguidade.\nCharlatanismo e hype em torno da IA se aproveitam da falta de compreensão técnica.\n\n\n\n\n\n\n\nO principal objetivo é trazer clareza conceitual para que diferentes atores da sociedade possam:\n\nEntender o que estão usando ou regulando.\nAvaliar riscos e benefícios com mais precisão.\nPromover inovação sem comprometer a segurança ou a ética.\nEvitar generalizações que geram medo, hype ou desinformação.\n\nA crescente presença da Inteligência Artificial em nossas vidas exige mais do que entusiasmo ou cautela, ela exige clareza conceitual. Ao nomearmos e distinguirmos os diferentes tipos de IA, abrimos caminho para um ecossistema mais transparente, seguro e produtivo. Essa distinção terá efeitos nas áreas técnicas, estratégicas e sociais.\n\n\n\nA necessidade surge da ambiguidade atual no uso do termo “IA”, que é aplicado tanto a sistemas simples como filtros de spam ou recomendação de músicas quanto a tecnologias avançadas como modelos generativos ou uso militar.\nEssa generalização além de confundir o público, pode dificultar a regulação, alimentar o sensacionalismo midiático além de permitir abusos comerciais e éticos. A classificação seria esclarecimento para proteção e potencialização.\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n    A[\"IA nível geral\"] --&gt; B1[\"IA simples e direta\"] & B2[\"IA intermediária e especializada\"] & B3[\"IA avançada e crítica\"] & B4[\"IA fronteira e disruptiva\"]\n    B1 --&gt; C1[\"Filtros de spam, recomendações, assistentes de voz básicos\"]\n    B2 --&gt; C2[\"Diagnóstico médico, crédito automatizado, logística\"]\n    B3 --&gt; C3[\"Modelos generativos, veículos autônomos, uso militar\"]\n    B4 --&gt; C4[\"AGI, IA autoevolutiva, protótipos experimentais\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    A[\"Classificação da IA\"] --&gt; B1[\"Organização Curricular\"] & B2[\"Formação por Competências\"] & B3[\"Ensino Ético e Crítico\"]\n    B1 --&gt; C1[\"IA Simples: Introdução à IA\"] & C2[\"IA Especializada: Aplicações Profissionais\"] & C3[\"IA Avançada: Pesquisa e Desenvolvimento\"]\n    B2 --&gt; D1[\"Habilidades Técnicas\"] & D2[\"Análise de Impacto Social\"] & D3[\"Comunicação Interdisciplinar\"]\n    B3 --&gt; E1[\"Discussões Éticas\"] & E2[\"Estudos de Caso\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    A[\"Classificação da IA\"] --&gt; B1[\"Análise de Risco\"] & B2[\"Criação de Normas\"] & B3[\"Monitoramento e Fiscalização\"]\n    B1 --&gt; C1[\"IA de Baixo Risco: Isenção Regulativa\"] & C2[\"IA de Médio Risco: Supervisão Moderada\"] & C3[\"IA de Alto Risco: Regulação Estrita\"]\n    B2 --&gt; D1[\"Normas Técnicas por Categoria\"] & D2[\"Diretrizes Éticas\"] & D3[\"Requisitos de Transparência\"]\n    B3 --&gt; E1[\"Auditorias Automatizadas\"] & E2[\"Relatórios de Conformidade\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    A[\"Classificação da IA\"] --&gt; B1[\"Posicionamento de Produto\"] & B2[\"Gestão de Riscos\"] & B3[\"Relacionamento com Stakeholders\"]\n    B1 --&gt; C1[\"Comunicação Clara com Clientes\"] & C2[\"Segmentação de Mercado\"] & C3[\"Evitar Overpromising\"]\n    B2 --&gt; D1[\"Mapeamento de Impacto\"] & D2[\"Compliance Tecnológico\"] & D3[\"Planejamento Estratégico\"]\n    B3 --&gt; E1[\"Transparência com Investidores\"] & E2[\"Educação do Consumidor\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    A[\"Classificação da IA\"] --&gt; B1[\"Reportagens Mais Precisas\"] & B2[\"Educação do Público\"] & B3[\"Combate à Desinformação\"]\n    B1 --&gt; C1[\"Especificar Tipo de IA\"] & C2[\"Evitar Sensacionalismo\"] & C3[\"Contextualizar Avanços\"]\n    B2 --&gt; D1[\"Glossários e Guias\"] & D2[\"Conteúdo Didático\"] & D3[\"Parcerias com Especialistas\"]\n    B3 --&gt; E1[\"Checagem de Fatos\"] & E2[\"Desmistificação de Tecnologias\"]\n\n\n\n\n\n\n\n\n\n\n\nApós a classificação da Inteligência Artificial, os próximos passos envolvem a criação de padrões e normas técnicas específicas para cada tipo de IA, permitindo uma aplicação mais segura e coerente da tecnologia.\nTambém se torna essencial o desenvolvimento de políticas públicas direcionadas, que considerem os diferentes níveis de risco e impacto social de cada categoria.\nParalelamente, é necessário investir na educação e capacitação tanto de profissionais quanto do público em geral, promovendo uma compreensão mais crítica e informada sobre o uso da IA.\nAs empresas, por sua vez, devem adotar uma postura de transparência ao comunicar o tipo de IA que utilizam, contribuindo para a confiança do mercado e dos consumidores.\nPor fim, o fomento à pesquisa ética e responsável em cada domínio da IA garante que o avanço tecnológico ocorra de forma alinhada aos valores sociais.\nA classificação, portanto, representa o ponto de partida para uma governança mais inteligente, justa e sustentável da tecnologia.\n\n\n\n\n\n\nBenefício: Compreensão acessível e realista sobre o que a IA pode (ou não) fazer.\nImpacto: Reduz o medo irracional e o fascínio exagerado. Permite decisões mais informadas no consumo de produtos e serviços baseados em IA.\nExemplo: Saber diferenciar um chatbot de um sistema de vigilância automatizado ajuda a entender riscos e utilidades reais.\n\n\n\n\n\nBenefício: Criação de políticas públicas mais precisas e eficazes.Leis e políticas mais eficazes, específicas e adaptadas ao risco\nImpacto: Evita legislações genéricas que travam a inovação ou deixam brechas perigosas.\nExemplo: Regulamentar IA em diagnósticos médicos exige critérios diferentes dos aplicáveis a IA em redes sociais.\n\n\n\n\n\nBenefício: Comunicação mais honesta com clientes, investidores e parceiros.inovação com responsabilidade e confiança do mercado.\nImpacto: Reduz o hype e o risco de promessas enganosas. Estimula a inovação responsável.\nExemplo: Uma empresa que desenvolve IA para logística pode se posicionar com mais clareza e confiança no mercado.\n\n\n\n\n\nBenefício: Currículos mais claros, formação crítica e alinhada com o mercado. Estruturação mais eficaz de currículos e materiais didáticos.\nImpacto: Facilita a formação de profissionais críticos e preparados para lidar com diferentes tipos de IA.\nExemplo: Ensinar IA generativa e IA simbólica como áreas distintas ajuda alunos a entenderem suas aplicações e limitações.\n\n\n\n\n\nBenefício: Reportagens mais precisas e educativas.\nImpacto: Combate a desinformação e o sensacionalismo. Promove o debate público qualificado.\nExemplo: Ao cobrir um avanço em IA, a mídia pode explicar se se trata de um sistema de linguagem, de visão computacional ou de automação industrial."
  },
  {
    "objectID": "posts/classificacao_ai/classificacao_ai.html#o-que-é-a-classificação-de-ia",
    "href": "posts/classificacao_ai/classificacao_ai.html#o-que-é-a-classificação-de-ia",
    "title": "classificacao_ai",
    "section": "",
    "text": "A classificação da Inteligência Artificial é o processo de organizar os diferentes tipos de sistemas de IA com base em critérios como função, complexidade, autonomia, impacto social e contexto de uso.\nAssim como classificamos tecnologias em categorias distintas (como softwares, veículos ou medicamentos) de acordo com função, complexidade e contexto de uso, podemos aplicar critérios semelhantes à IA, que com uma taxonomia clara, possa ser melhor compreendida e aplicada de forma responsável.\n\n\n\n\n\n\n\n\n\nVeículo (analogia)\nInteligência Artificial (realidade)\n\n\n\n\nPalavra genérica que engloba carros, bicicletas, ônibus, foguetes, etc.\nTermo guarda-chuva que inclui desde chatbots simples até sistemas autônomos complexos.\n\n\nDiscussões confusas sobre “veículos” sem distinguir entre tipos.\nDebates sobre IA (ética, impacto, regulação) sem diferenciar entre modelos e aplicações.\n\n\nAvanços em foguetes confundidos com melhorias em carros.\nAvanços em IA generativa confundidos com IA de recomendação ou automação industrial.\n\n\nGolpes e desinformação prosperam na ambiguidade.\nCharlatanismo e hype em torno da IA se aproveitam da falta de compreensão técnica."
  },
  {
    "objectID": "posts/classificacao_ai/classificacao_ai.html#qual-o-objetivo-da-classificação-de-ia",
    "href": "posts/classificacao_ai/classificacao_ai.html#qual-o-objetivo-da-classificação-de-ia",
    "title": "classificacao_ai",
    "section": "",
    "text": "O principal objetivo é trazer clareza conceitual para que diferentes atores da sociedade possam:\n\nEntender o que estão usando ou regulando.\nAvaliar riscos e benefícios com mais precisão.\nPromover inovação sem comprometer a segurança ou a ética.\nEvitar generalizações que geram medo, hype ou desinformação.\n\nA crescente presença da Inteligência Artificial em nossas vidas exige mais do que entusiasmo ou cautela, ela exige clareza conceitual. Ao nomearmos e distinguirmos os diferentes tipos de IA, abrimos caminho para um ecossistema mais transparente, seguro e produtivo. Essa distinção terá efeitos nas áreas técnicas, estratégicas e sociais."
  },
  {
    "objectID": "posts/classificacao_ai/classificacao_ai.html#de-onde-vem-a-necessidade-da-classificação-de-ia",
    "href": "posts/classificacao_ai/classificacao_ai.html#de-onde-vem-a-necessidade-da-classificação-de-ia",
    "title": "classificacao_ai",
    "section": "",
    "text": "A necessidade surge da ambiguidade atual no uso do termo “IA”, que é aplicado tanto a sistemas simples como filtros de spam ou recomendação de músicas quanto a tecnologias avançadas como modelos generativos ou uso militar.\nEssa generalização além de confundir o público, pode dificultar a regulação, alimentar o sensacionalismo midiático além de permitir abusos comerciais e éticos. A classificação seria esclarecimento para proteção e potencialização."
  },
  {
    "objectID": "posts/classificacao_ai/classificacao_ai.html#como-fazer-a-classificação-da-ia",
    "href": "posts/classificacao_ai/classificacao_ai.html#como-fazer-a-classificação-da-ia",
    "title": "classificacao_ai",
    "section": "",
    "text": "flowchart TD\n    A[\"IA nível geral\"] --&gt; B1[\"IA simples e direta\"] & B2[\"IA intermediária e especializada\"] & B3[\"IA avançada e crítica\"] & B4[\"IA fronteira e disruptiva\"]\n    B1 --&gt; C1[\"Filtros de spam, recomendações, assistentes de voz básicos\"]\n    B2 --&gt; C2[\"Diagnóstico médico, crédito automatizado, logística\"]\n    B3 --&gt; C3[\"Modelos generativos, veículos autônomos, uso militar\"]\n    B4 --&gt; C4[\"AGI, IA autoevolutiva, protótipos experimentais\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    A[\"Classificação da IA\"] --&gt; B1[\"Organização Curricular\"] & B2[\"Formação por Competências\"] & B3[\"Ensino Ético e Crítico\"]\n    B1 --&gt; C1[\"IA Simples: Introdução à IA\"] & C2[\"IA Especializada: Aplicações Profissionais\"] & C3[\"IA Avançada: Pesquisa e Desenvolvimento\"]\n    B2 --&gt; D1[\"Habilidades Técnicas\"] & D2[\"Análise de Impacto Social\"] & D3[\"Comunicação Interdisciplinar\"]\n    B3 --&gt; E1[\"Discussões Éticas\"] & E2[\"Estudos de Caso\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    A[\"Classificação da IA\"] --&gt; B1[\"Análise de Risco\"] & B2[\"Criação de Normas\"] & B3[\"Monitoramento e Fiscalização\"]\n    B1 --&gt; C1[\"IA de Baixo Risco: Isenção Regulativa\"] & C2[\"IA de Médio Risco: Supervisão Moderada\"] & C3[\"IA de Alto Risco: Regulação Estrita\"]\n    B2 --&gt; D1[\"Normas Técnicas por Categoria\"] & D2[\"Diretrizes Éticas\"] & D3[\"Requisitos de Transparência\"]\n    B3 --&gt; E1[\"Auditorias Automatizadas\"] & E2[\"Relatórios de Conformidade\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    A[\"Classificação da IA\"] --&gt; B1[\"Posicionamento de Produto\"] & B2[\"Gestão de Riscos\"] & B3[\"Relacionamento com Stakeholders\"]\n    B1 --&gt; C1[\"Comunicação Clara com Clientes\"] & C2[\"Segmentação de Mercado\"] & C3[\"Evitar Overpromising\"]\n    B2 --&gt; D1[\"Mapeamento de Impacto\"] & D2[\"Compliance Tecnológico\"] & D3[\"Planejamento Estratégico\"]\n    B3 --&gt; E1[\"Transparência com Investidores\"] & E2[\"Educação do Consumidor\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    A[\"Classificação da IA\"] --&gt; B1[\"Reportagens Mais Precisas\"] & B2[\"Educação do Público\"] & B3[\"Combate à Desinformação\"]\n    B1 --&gt; C1[\"Especificar Tipo de IA\"] & C2[\"Evitar Sensacionalismo\"] & C3[\"Contextualizar Avanços\"]\n    B2 --&gt; D1[\"Glossários e Guias\"] & D2[\"Conteúdo Didático\"] & D3[\"Parcerias com Especialistas\"]\n    B3 --&gt; E1[\"Checagem de Fatos\"] & E2[\"Desmistificação de Tecnologias\"]"
  },
  {
    "objectID": "posts/classificacao_ai/classificacao_ai.html#pra-onde-vai-próximos-passos-após-a-classificação-da-ia",
    "href": "posts/classificacao_ai/classificacao_ai.html#pra-onde-vai-próximos-passos-após-a-classificação-da-ia",
    "title": "classificacao_ai",
    "section": "",
    "text": "Após a classificação da Inteligência Artificial, os próximos passos envolvem a criação de padrões e normas técnicas específicas para cada tipo de IA, permitindo uma aplicação mais segura e coerente da tecnologia.\nTambém se torna essencial o desenvolvimento de políticas públicas direcionadas, que considerem os diferentes níveis de risco e impacto social de cada categoria.\nParalelamente, é necessário investir na educação e capacitação tanto de profissionais quanto do público em geral, promovendo uma compreensão mais crítica e informada sobre o uso da IA.\nAs empresas, por sua vez, devem adotar uma postura de transparência ao comunicar o tipo de IA que utilizam, contribuindo para a confiança do mercado e dos consumidores.\nPor fim, o fomento à pesquisa ética e responsável em cada domínio da IA garante que o avanço tecnológico ocorra de forma alinhada aos valores sociais.\nA classificação, portanto, representa o ponto de partida para uma governança mais inteligente, justa e sustentável da tecnologia."
  },
  {
    "objectID": "posts/classificacao_ai/classificacao_ai.html#quais-os-resultados-para-cada-grupo",
    "href": "posts/classificacao_ai/classificacao_ai.html#quais-os-resultados-para-cada-grupo",
    "title": "classificacao_ai",
    "section": "",
    "text": "Benefício: Compreensão acessível e realista sobre o que a IA pode (ou não) fazer.\nImpacto: Reduz o medo irracional e o fascínio exagerado. Permite decisões mais informadas no consumo de produtos e serviços baseados em IA.\nExemplo: Saber diferenciar um chatbot de um sistema de vigilância automatizado ajuda a entender riscos e utilidades reais.\n\n\n\n\n\nBenefício: Criação de políticas públicas mais precisas e eficazes.Leis e políticas mais eficazes, específicas e adaptadas ao risco\nImpacto: Evita legislações genéricas que travam a inovação ou deixam brechas perigosas.\nExemplo: Regulamentar IA em diagnósticos médicos exige critérios diferentes dos aplicáveis a IA em redes sociais.\n\n\n\n\n\nBenefício: Comunicação mais honesta com clientes, investidores e parceiros.inovação com responsabilidade e confiança do mercado.\nImpacto: Reduz o hype e o risco de promessas enganosas. Estimula a inovação responsável.\nExemplo: Uma empresa que desenvolve IA para logística pode se posicionar com mais clareza e confiança no mercado.\n\n\n\n\n\nBenefício: Currículos mais claros, formação crítica e alinhada com o mercado. Estruturação mais eficaz de currículos e materiais didáticos.\nImpacto: Facilita a formação de profissionais críticos e preparados para lidar com diferentes tipos de IA.\nExemplo: Ensinar IA generativa e IA simbólica como áreas distintas ajuda alunos a entenderem suas aplicações e limitações.\n\n\n\n\n\nBenefício: Reportagens mais precisas e educativas.\nImpacto: Combate a desinformação e o sensacionalismo. Promove o debate público qualificado.\nExemplo: Ao cobrir um avanço em IA, a mídia pode explicar se se trata de um sistema de linguagem, de visão computacional ou de automação industrial."
  }
]